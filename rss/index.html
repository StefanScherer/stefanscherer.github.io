<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Stefan Scherer's Blog]]></title><description><![CDATA[Just my techie notes.]]></description><link>https://stefanscherer.github.io/</link><image><url>https://stefanscherer.github.io/favicon.png</url><title>Stefan Scherer&apos;s Blog</title><link>https://stefanscherer.github.io/</link></image><generator>Ghost 1.8</generator><lastBuildDate>Wed, 27 May 2020 09:29:33 GMT</lastBuildDate><atom:link href="https://stefanscherer.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Take Five: My favorite DockerCon LIVE 2020 talks]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Tomorrow the first virtual DockerCon is going to happen. As a former Docker Captain I'm really excited about the event. Of course I'll start my day a bit later as DockerCon starts at 6PM my time. I went through <a href="https://docker.events.cube365.net/docker/dockercon/agenda">the DockerCon agenda</a> and found a few highlights I want to</p></div>]]></description><link>https://stefanscherer.github.io/my-favorite-dockercon-2020-talks/</link><guid isPermaLink="false">5ece1fca8b847a000138772f</guid><category><![CDATA[DockerCon]]></category><category><![CDATA[Docker]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Wed, 27 May 2020 08:39:56 GMT</pubDate><media:content url="https://stefanscherer.github.io/content/images/2020/05/Screen-Shot-2020-05-27-at-10.44.39-AM.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="https://stefanscherer.github.io/content/images/2020/05/Screen-Shot-2020-05-27-at-10.44.39-AM.png" alt="Take Five: My favorite DockerCon LIVE 2020 talks"><p>Tomorrow the first virtual DockerCon is going to happen. As a former Docker Captain I'm really excited about the event. Of course I'll start my day a bit later as DockerCon starts at 6PM my time. I went through <a href="https://docker.events.cube365.net/docker/dockercon/agenda">the DockerCon agenda</a> and found a few highlights I want to attend and want to recommend to you.</p>
<h2 id="1howtocreatephpdevelopmentenvironmentswithdockercompose">1. How to Create PHP Development Environments with Docker Compose</h2>
<p>I know Erika Heidi <a href="https://twitter.com/erikaheidi">@erikaheidi</a> back from Vagrant and her book. That's why I'm excited to see how she uses <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/zpLCJq3m8Jn5tQNCi">Docker Compose for PHP development</a>.</p>
<h2 id="2bestpracticesforcomposemanagedpythonapplications">2. Best Practices for Compose-managed Python Applications</h2>
<p>And there is another interesting session about Docker Compose for local development. Anca Lordache from Docker shows <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/eWWPtj5dmHAmoYypc">best practices for Python</a>.  But I guess you can use the same practices for other language stacks as well.</p>
<h2 id="3handsonhelm">3. Hands-on Helm</h2>
<p>I'm interested in how people deploy their applications in Kubernetes. Helm is the defacto package manager. Jessica Deen <a href="https://twitter.com/jldeen">@jldeen</a> from Microsoft gives you an overview in <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/ZbxZrH75SKqf78x2S">Hands-on Helm</a> and shows how you can update to Helm 3.</p>
<h2 id="4deliveringdesktopappsincontainers">4. Delivering Desktop Apps in Containers</h2>
<p>What? That sounds interesting and I expect a fascinating talk how to do that,  because containers are normally for backend and non GUI applications. Blaize Stewart <a href="https://twitter.com/theonemule">@theonemule</a>, a Mircosoft MVP, is showing the details how to <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/wLMYLQAYj7mvHrvZe">run GUI applications in containers</a>.</p>
<h2 id="5dockerdesktopwsl2integrationdeepdive">5. Docker Desktop + WSL 2 Integration Deep Dive</h2>
<p>Simon Ferquel <a href="https://twitter.com/sferquel">@sferquel</a> from Docker does a <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/K5D8qKJpX658yY8o9">deep dive into the new WSL 2 backend</a>, how it works, and how to get the most out of Docker Desktop on Windows. If you want to see how Docker Desktop works that's a session for you.</p>
<p>And there are many more excellent sessions. If you are new to Docker, we have you covered, Peter McKee gives you an introduction <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/hgMFTyX5kYKmTPWZo">how you get started</a>. Looking into Windows Containers? Elton Stoneman's talk will be <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/xxCByyu83JtCHwNHH">your favorite</a>.</p>
<h2 id="captainsondeck">Captains on Deck</h2>
<p>I will also hang around with the <a href="https://docker.events.cube365.net/docker/dockercon/content/Videos/jFNDwpTbHotrtbt5x">Docker Captains</a> in the chat. It is always inspiring what Captains do and they can answer your questions in parallel to the other sessions. Join Bret Fisher and many other Captains for an entertaining chat and win some prizes.</p>
<h2 id="callforaction">Call for action</h2>
<p>Just register <a href="https://docker.events.cube365.net/docker/dockercon/">to attend DockerCon</a> and pick your favorite session from the <a href="https://docker.events.cube365.net/docker/dockercon/agenda">agenda</a>.</p>
<p>See you at #DockerCon !!</p>
<p>Stefan</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to configure secrets in Azure Pipelines]]></title><description><![CDATA[<div class="kg-card-markdown"><p>A few weeks ago I tried Azure Pipelines for one of <a href="https://github.com/StefanScherer/azurepipelinetest">my GitHub repos</a>. Azure Pipelines is a cloud service to setup CI/CD pipelines. I'm already using Travis, CircleCI and AppVeyor for years, but I wanted to give Azure Pipelines a try to see how it has evolved.</p>
<p>It's</p></div>]]></description><link>https://stefanscherer.github.io/how-to-use-secrets-in-azure-pipelines/</link><guid isPermaLink="false">5cd6ebf93e10bf000199b6e7</guid><category><![CDATA[Azure]]></category><category><![CDATA[secrets]]></category><category><![CDATA[pipeline]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sat, 11 May 2019 17:22:27 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>A few weeks ago I tried Azure Pipelines for one of <a href="https://github.com/StefanScherer/azurepipelinetest">my GitHub repos</a>. Azure Pipelines is a cloud service to setup CI/CD pipelines. I'm already using Travis, CircleCI and AppVeyor for years, but I wanted to give Azure Pipelines a try to see how it has evolved.</p>
<p>It's very easy to hook it to your <a href="https://github.com/marketplace/azure-pipelines/plan/MDIyOk1hcmtldHBsYWNlTGlzdGluZ1BsYW4xMTkz#pricing-and-setup">GitHub project</a>, and it's free for open source projects. You typically add an <a href="https://github.com/StefanScherer/azurepipelinetest/blob/printenvtest/azure-pipelines.yml">azure-pipelines.yml</a> in your GitHub repo to define the CI pipeline. Then this definition is also under version control. And users can easily fork your repo and hook up their own pipeline this the given YML file.</p>
<p>For a real project to build and push a <a href="https://github.com/StefanScherer/azurepipelinetest/blob/master/azure-pipelines.yml">multi-arch Windows/Linux Docker image</a> I tried to use secret variables for the <code>docker push</code>. But I struggled for an hour <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&amp;tabs=yaml%2Cbatch#secret-variables">reading the docs</a> again and again and compare it with the actual UI I found from my MacBook. I just couldn't understand where this tiny lock icon should be. I added variables, removed them again, search for other options. Nothing. Is it only available in paid plan? All these thoughts to figure out when you don't have a clue where the developer has added this button.</p>
<p>Look at my screen:</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/05/azure-pipelines-variables-where-is-the-lock.jpg" alt="azure pipelines variables where is the lock"></p>
<p>No lock icon.</p>
<p>I don't know when it happened, but I &quot;accidentally&quot; two-finger-swiped on the table of variables.</p>
<p>Oh no, so close but it's really not intuitive for first-time users with a non-24&quot; external display sitting on a chair in the conversatory :-)</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/05/azure-pipelines-variables-here-my-lock.jpg" alt="azure pipelines variables here my lock"></p>
<p>Now I know where to look for that lock icon, but please MSFT let the first-time users know better where to put secrets. Otherwise they just add variables that should be secrets but as I just ran a test if it works that forked pull requests don't see these secrets. No the user of the fork can see this variable until you click on that &quot;hidden&quot; lock icon.</p>
<p>There is so much white space that can be used to have this lock column visible even on smaller displays.</p>
<p>The rest of the pipeline was straight forward and Azure Pipelines is able to build and push a <a href="https://hub.docker.com/r/stefanscherer/hello">multi-arch image to Docker Hub</a> for several Windows versions as well as Linux for different CPU architectures with some tricks. Thanks Azure Pipelines team, you rock, I like Azure Pipelines so far.</p>
<p>If you want to give Microsoft feedback, you can <a href="https://developercommunity.visualstudio.com/idea/531856/azure-pipelines-variables-bad-ux-to-find-lock-for.html?childToView=565168#comment-565168">vote up my comment here</a>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[Joining Docker]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Today is my first day working for Docker, Inc. and I'm absolutely excited to be there. After months of prepartions I got this email today morning.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/github-docker-join.png" alt="github-docker-join"></p>
<p>I love the green buttons on GitHub ;-)</p>
<h1 id="goodbyesealsystems">Goodbye SEAL Systems</h1>
<p>After nearly 25 years (I started as a working student in 1994) I</p></div>]]></description><link>https://stefanscherer.github.io/joining-docker/</link><guid isPermaLink="false">5c5404f4c5e8170001f8ab5e</guid><category><![CDATA[Docker]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Fri, 01 Feb 2019 09:30:49 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Today is my first day working for Docker, Inc. and I'm absolutely excited to be there. After months of prepartions I got this email today morning.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/github-docker-join.png" alt="github-docker-join"></p>
<p>I love the green buttons on GitHub ;-)</p>
<h1 id="goodbyesealsystems">Goodbye SEAL Systems</h1>
<p>After nearly 25 years (I started as a working student in 1994) I want to say goodbye to my colleages at <a href="https://www.sealsystems.de">SEAL Systems</a>. Wow, that's more than half of my life, but we always had a good time. I'm glad that I could help creating several successful <a href="https://www.sealsystems.de/output-management/cloud/cloud-printing/">products</a> over these years. I have learned a lot and the whole lifecycle of a product is much much more than just writing some lines of code. You have to build a product, ship it, update and maintain it, replace it with a better, newer product. The enterprise customers are always demanding.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/goodbye-seal.jpg" alt="goodbye-seal"></p>
<p>And it was fun to build this <a href="https://github.com/sealsystems/tiny-cloud">tiny cloud</a> to visualize scaling services and health status in hardware, and of course producing paper and labels.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/captain-at-work.jpg" alt="captain-at-work"></p>
<h1 id="goodbyecaptains">Goodbye Captains</h1>
<p>Oh this is a sad one, I have to retire from the <a href="https://www.docker.com/community/captains">Docker Captains</a> program. I want to thank <a href="https://twitter.com/TheBurce">Jenny</a>, Vanessa and <a href="https://twitter.com/AshlynnPolini">Ashlynn</a> for making us Captains happy with briefings, lots of <a href="https://twitter.com/BretFisher/status/1071185259541225473">cool swag</a> and exciting <a href="https://blog.hypriot.com/post/dockerconaustin2017/">DockerCon</a>'s. You make this program unique!</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/Docker-Captain-Team-Photo.jpg" alt="Docker-Captain-Team-Photo"></p>
<p>I will stay in touch with a lot of captains as I still can learn cool things from them.</p>
<h2 id="connectingthedots">Connecting the dots</h2>
<p>Yesterday was my last day at SEAL Systems and I realized that this move just made sense for me. Even the logos showed me that :-) I follow what started with my passion and what I want to focus on in the future.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/02/seal-docker-033.gif" alt="seal-docker-033"></p>
<p>(And for SEAL: You always had containers in your logo ;-)</p>
<p>I will work in the Engineering team at Docker to help shipping new products.<br>
Dear community, don't worry I will stay active in the Windows and Docker community.</p>
<p>Cheers,<br>
Stefan</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to run lightweight Windows Containers on Windows 10]]></title><description><![CDATA[<div class="kg-card-markdown"><p>When you follow my blog for a while you probably know that running Windows Containers on Windows 10 had some disadvantages compared to a Windows Server. On Windows 10 every Windows Containers has to be run in Hyper-V isolation mode.</p>
<h2 id="processisolation">Process Isolation</h2>
<p>With the latest release of Docker Desktop on</p></div>]]></description><link>https://stefanscherer.github.io/how-to-run-lightweight-windows-containers-on-windows-10/</link><guid isPermaLink="false">5c401c9fc5e8170001f8ab51</guid><category><![CDATA[Windows 10]]></category><category><![CDATA[windows-containers]]></category><category><![CDATA[Docker Desktop]]></category><category><![CDATA[Docker]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Fri, 18 Jan 2019 08:23:21 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>When you follow my blog for a while you probably know that running Windows Containers on Windows 10 had some disadvantages compared to a Windows Server. On Windows 10 every Windows Containers has to be run in Hyper-V isolation mode.</p>
<h2 id="processisolation">Process Isolation</h2>
<p>With the latest release of Docker Desktop on Windows 10 1809 you now can run Windows Containers in process isolation mode. What's the benefit you might think.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/01/docker-desktop-2-0-0-2.png" alt="docker-desktop-2-0-0-2"></p>
<p>In the past process isolation was only possible with Windows Server. The Windows 10 operating system uses the same kernel, but with different settings. With this pull request <a href="https://github.com/moby/moby/pull/38000">https://github.com/moby/moby/pull/38000</a> that got merged into Docker 18.09.1 it is now possible to use it on Windows 10 as well.</p>
<ul>
<li>You can start more Windows Containers on your machine as they consume less resources</li>
<li>Containers normally start faster than in hyperv isolation mode</li>
<li>You can &quot;see&quot; the isolated processes and what they are doing</li>
</ul>
<h2 id="visiblecontainerprocesses">Visible container processes</h2>
<p>Especially for developers this is a great enhancement, because you now can use tools like Task Manager, Process Monitor and others to inspect your container processes from the host. I've blogged <a href="https://stefanscherer.github.io/find-dependencies-in-windows-containers/">How to find dependencies of containerized Windows apps</a> about a year ago. Now you do not longer need to spin up a Windows Server VM to do that, your Windows 10 machine is all you need.</p>
<p>Let's try this out with a small web server I have created for the <a href="https://chocolateyfest.com">Chocolatey Fest</a> conference last October that's running in a Windows Nanoserver 2019 container.</p>
<p>Open up a PowerShell terminal and start a Windows container with this command</p>
<pre><code>docker run -d -p 8080:8080 --isolation=process chocolateyfest/appetizer:1.0.0
</code></pre>
<p>The command will pull the Docker image from <a href="https://hub.docker.com/r/chocolateyfest/appetizer">Docker Hub</a>, starts the web server as a container and forwards port 8080 to it.</p>
<p>Now you can access the web server with your browser or by typing this command</p>
<pre><code>start http://localhost:8080
</code></pre>
<p>The web server should show you a sweet photo and the name of the container stamped on it.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/01/windows-10-process-isolation.png" alt="windows-10-process-isolation"></p>
<p>As you can see in the screen shot you can see the <code>node.exe</code> process in the Task Manager. If you have the Sysinternals Process Monitor installed you also can see what the containerized process is doing. This is great when you create an own Docker image from your or a 3rd-party app and something doesn't work as expected or the exe file just doesn't want to start inside the container.</p>
<h2 id="windowsimageversionmustmatchkernelversion">Windows image version must match kernel version</h2>
<p>The only caveat using the process isolation mode is that the Windows base image that is used for a Docker image must match the kernel of your Windows 10 machine.</p>
<p>I've tried process isolation on a Windows Insider 18xxx machine, but here you are out of luck and you have to run the 1809 images in default Hyper-V isolation mode.</p>
<h2 id="canirunwindowscontainersinvirtualbox">Can I run Windows Containers in VirtualBox?</h2>
<p>I run all these tests in VMware Fusion on my Mac, spinning up a <a href="https://github.com/StefanScherer/windows-docker-desktop-box">Windows 10 1809 VM with Vagrant</a>. You can try it yourself with the given <code>Vagrantfile</code> in the repo.</p>
<p>For a full Docker Desktop experience you need VMware Fusion as it provides nested virtualization. This is needed to activate Hyper-V in the Windows 10 VM. Docker Desktop runs fine in that VMware VM and you can try out Linux and Windows containers in it.</p>
<p>From time to time I get asked if people can also use VirtualBox. In the past I had to say &quot;no&quot; you can't use a Windows 10 VM and then run Windows Containers in it. But with process isolation there is a first breakthrough.</p>
<h3 id="dangerzoneforearlyadopters">Danger zone for early adopters</h3>
<p>I've tried that with VirtualBox to see what happens. The installation of Docker Desktop works without a problem. When you start Docker Desktop for the first time the following error will appear</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/01/win10-virtualbox-linux-error.png" alt="win10-virtualbox-linux-error"></p>
<p>Sure, Hyper-V does not work in a VirtualBox VM, that's why the MobyLinuxVM could not be started. But now you can switch to Windows containers in the context menu.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/01/win10-switch-to-windows-container.png" alt="win10-switch-to-windows-container"></p>
<p>After a few seconds the Windows Docker engine is up and running. Open a PowerShell terminal and run the appetizer app as described above.</p>
<p><img src="https://stefanscherer.github.io/content/images/2019/01/win10-virtualbox-windows-container.png" alt="win10-virtualbox-windows-container"></p>
<p>Voila! It works.</p>
<p>Try something different with an interactive nanoserver container with a CMD shell</p>
<pre><code>docker run -it --isolation=process mcr.microsoft.com/windows/nanoserver:1809 cmd
</code></pre>
<h2 id="tldr">TL/DR</h2>
<p>Beginning with Windows 10 1809 and Docker 18.09.1 you can use the more lightweight process isolation mode for Windows Containers. Linux Containers still need Hyper-V installed to run them in Docker Desktop.</p>
<p>If you liked this blog post please share it with your friends. You can follow me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to install Docker the Chocolatey way]]></title><description><![CDATA[<div class="kg-card-markdown"><p>When I'm working with Windows I love to have a standarized way to install software. Did you remember how we have set up our dev machines a few years ago? Well, about five years ago I found this blog post by security expert Troy Hunt and his <a href="https://www.troyhunt.com/102-simple-steps-for-installing-and/">102 simple steps</a></p></div>]]></description><link>https://stefanscherer.github.io/how-to-install-docker-the-chocolatey-way/</link><guid isPermaLink="false">5bff8621e0fd760001c92558</guid><category><![CDATA[Docker]]></category><category><![CDATA[Chocolatey]]></category><category><![CDATA[Windows 7]]></category><category><![CDATA[Windows 10]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Fri, 30 Nov 2018 07:49:23 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>When I'm working with Windows I love to have a standarized way to install software. Did you remember how we have set up our dev machines a few years ago? Well, about five years ago I found this blog post by security expert Troy Hunt and his <a href="https://www.troyhunt.com/102-simple-steps-for-installing-and/">102 simple steps for installing and configuring a new Windows 8 machine</a> showed most of the time <code>cinst this</code> and <code>cinst that</code>. This opened my eyes, wow there is a package manager for Windows. Since then I started with automation tools like <a href="https://www.packer.io">Packer</a> and <a href="https://www.vagrantup.com">Vagrant</a> to describe repeatable development and test environments. This also lead me to contribute back to the Chocolatey community repository, because I just couldn't <code>cinst packer</code> at that time. So I wrote my <a href="https://chocolatey.org/packages/packer/0.3.8">first Chocolatey package</a> which is very easy as it only links to the official download URL's from the software vendor.</p>
<p>In these five years I went through several Windows machines and contributed missing Choco packages also for installing the Docker tools I needed.</p>
<h2 id="overview">Overview</h2>
<p>The following diagram shows you the most relevant Chocolatey packages for Docker. I'll give you a little bit of history and explain why they all exist in the following chapters.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/11/windows-docker-desktop.jpg" alt="Overview of Docker chocolatey packages"></p>
<h2 id="docker">Docker</h2>
<p>The first Docker tool that landed as a Chocolatey package was the Docker CLI. <a href="https://twitter.com/ahmetb">Ahmet Alp Balkan</a> working at Microsoft at that time ported the Docker CLI to Windows so we had the <code>docker.exe</code> to communicate with remote Docker engines running in a Linux machine. This package was and still is called <code>docker</code>.</p>
<p>Nowadays it might be confusing if people want to run <code>choco install docker</code> and 'just' get the Docker CLI without any Docker Engine. We're in discussion with the Chocolatey team how to softly fix this and transfer the Docker CLI into a new package name called <code>docker-cli</code> to make it more clear.</p>
<h2 id="dockertoolbox">Docker Toolbox</h2>
<p>Docker, Inc. created Docker Toolbox to have all tools and also VirtualBox bundled together. <a href="https://twitter.com/MRiezebosch">Manuel Riezebosch</a> started a Chocolatey package <code>docker-toolbox</code> for it and still maintains it.<br>
This package is usable for people that cannot run the newer Docker Desktop product. The reasons could be</p>
<ul>
<li>Still running Windows 7</li>
<li>Running a Windows 10 Home or LTSB version which is too old</li>
<li>Running VirtualBox VM's for other tasks that prevent the installation of Hyper-V</li>
</ul>
<h2 id="machinecompose">Machine, Compose, ...</h2>
<p>I worked with VMware Workstation for years so the Docker Toolbox <a href="https://stefanscherer.github.io/yes-you-can-docker-on-windows-7/">wasn't my thing</a>. I knew that there is a tool called <code>docker-machine</code> to create Linux VM's with the boot2docker.iso file. That's why I started with the Choco packages for <code>docker-machine</code>, helped maintaining the <code>docker-compose</code> package and added some Docker Machine drivers as Chocolatey packages <code>docker-machine-vmwareworkstation</code> and <code>docker-machine-vmware</code> as well.</p>
<p>This is the fine granular approach to install only the tools you need, but still using the <code>choco install</code> experience.</p>
<h2 id="dockerforwindows">Docker for Windows</h2>
<p><a href="https://twitter.com/MRiezebosch">Manuel Riezebosch</a> started a Chocolatey package <code>docker-for-windows</code> which is an excellent work. You can install &quot;Docker for Windows&quot; product with it which is the successor of &quot;Docker Toolbox&quot;. But please read the next section to grab the latest version of it.</p>
<h2 id="dockerdesktop">Docker Desktop</h2>
<p>With the new release of Docker Desktop 2.0 for Windows 10 Pro/Enterprise there is also a change in the name. The product &quot;Docker for Windows&quot; has been renamed to &quot;Docker Desktop&quot;. It also gets a new version format.</p>
<p>That's the reason to start with a new Choco package name. Please unlearn <code>docker-for-windows</code> and just use <code>choco install docker-desktop</code> to get the latest version on your machine.</p>
<ul>
<li><a href="https://chocolatey.org/packages/docker-desktop">https://chocolatey.org/packages/docker-desktop</a></li>
</ul>
<p>Thanks <a href="https://twitter.com/MRiezebosch">Manuel Riezebosch</a> for mainting this choco package!</p>
<h2 id="windowsserver">Windows Server?</h2>
<p>If you want to install Docker on a Windows Server 2016 or 2019, there is no Chocolatey package for it.</p>
<p>Please read <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/quick-start-windows-server">Windows Containers on Windows Server</a> installation guide from Microsoft or the <a href="https://store.docker.com/editions/enterprise/docker-ee-server-windows">Docker Enterprise Edition for Windows Server</a> guide from the Docker Store.</p>
<h2 id="tldr">TL/DR</h2>
<p>The best experience with Docker on a Windows 10 machine is using the Docker Desktop product. Try to grab an up-to-date Windows 10 Pro machine to be all set for it and then run</p>
<pre><code>choco install docker-desktop
</code></pre>
<p>Otherwise jump over to <a href="https://chocolatey.org/search?q=docker">https://chocolatey.org/search?q=docker</a> and grab one of the other Docker related Chocolatey packages.</p>
<p>I hope this overview of all the Chocolatey packages will give you a better understanding of what is right for your needs. I would love to hear your feedback so please leave a comment below or ask me on <a href="https://twitter.com/stefscherer">Twitter</a>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[What's new for Docker on Windows Server 2019?]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Last week at MS Ignite Microsoft has announced the new Windows Server 2019 which will be general available in October. This is a big new release with a lot of improvements using Docker with Windows Containers. Here is an overview of relevant changes.</p>
<p>Since the last two years after Windows</p></div>]]></description><link>https://stefanscherer.github.io/docker-on-windows-server-2019/</link><guid isPermaLink="false">5ba63112078c7c0001c0a3e8</guid><category><![CDATA[Docker]]></category><category><![CDATA[Windows Server 2019]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Wed, 03 Oct 2018 14:00:00 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Last week at MS Ignite Microsoft has announced the new Windows Server 2019 which will be general available in October. This is a big new release with a lot of improvements using Docker with Windows Containers. Here is an overview of relevant changes.</p>
<p>Since the last two years after Windows Server 2016 first introduced Windows Container support a lot of things have improved. We have seen some of that changes in the semi-annual releases of 1709 and 1803, and now the long-term supported release has all the latest and greatest updates available.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/10/Bildschirmfoto-2018-10-03-um-11.18.17.png" alt="Windows Server 2019 with Docker installed"></p>
<h2 id="inplaceupdatefrom2012r22016">In-Place update from 2012 R2 / 2016</h2>
<p>First of all if you have older Windows Servers running it is possible to install an in-place update to this new release. So it is possible to run Windows Containers after updating your server, adding the <strong>Containers feature</strong> and <strong>installing Docker</strong> on your server. I normally create fresh machines with the new operating system, but this update really looks interesting to get access to these new technology.</p>
<h2 id="smallerbasedockerimages">Smaller base Docker images</h2>
<p>The containers team at Microsoft has improved the size of the Windows base images. The container images have been shrunk down to 1/3 to 1/4 of the equivalent 2016 images. The sizes in the diagram below are the sizes after downloading and expanding the Docker images and running the <code>docker images</code> command.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/10/Bildschirmfoto-2018-10-03-um-15.46.33.png" alt="Comparison of Docker image sizes for 2016 and 2019"></p>
<p>With the very small <code>mcr.microsoft.com/windows/nanoserver:1809</code> image you will see applications at about 100 MByte (compressed) on Docker Hub.</p>
<h2 id="anewwindowsbaseimage">A new windows base image</h2>
<p>In addition to the two known Windows base images for <strong>Windows Server Core</strong> and <strong>Nano Server</strong> there is now a third base image: <strong>Windows</strong></p>
<p><img src="https://stefanscherer.github.io/content/images/2018/10/Bildschirmfoto-2018-10-03-um-16.03.46.png" alt="Three Windows base OS images"></p>
<p>This image gives you an even broader support for your Windows applications than just the core image. One use-case is for automation workloads like automated UI tests. But notice you still cannot RDP into such Windows containers.</p>
<h2 id="transitiontomcrmicrosoftcom">Transition to <code>mcr.microsoft.com</code></h2>
<p>Microsoft has started to move its Docker images from the Docker Hub into a own container registry. What you have to know is that the name for the base images will slightly change. You only have to remember to change the <code>microsoft/</code> to <code>mcr.microsoft.com/</code>. The following example shows the old and the new image name. Watch out the additional slash <code>/</code> for the Windows Server Core image.</p>
<pre><code>FROM microsoft/windowsservercore:ltsc2016
</code></pre>
<p>to</p>
<pre><code>FROM mcr.microsoft.com/windows/servercore:ltsc2019
</code></pre>
<p>The tags on Docker Hub are still there and you still will be able to pull the images with the old image name for a while.</p>
<p>The Windows base images has always been hosted on Microsoft CDN as &quot;foreign layers&quot;, so really only the tag names changes.</p>
<p>A good question is where can you find the new images. The <code>mcr.microsoft.com</code> registry does not have an UI.</p>
<p>At the time of writing this blog post the new Docker images are not available. The latest information that can be found is on Docker Hub for the Insider images:</p>
<p><a href="https://hub.docker.com/r/microsoft/nanoserver-insider/">https://hub.docker.com/r/microsoft/nanoserver-insider/</a><br>
<a href="https://hub.docker.com/r/microsoft/windowsservercore-insider/">https://hub.docker.com/r/microsoft/windowsservercore-insider/</a><br>
<a href="https://hub.docker.com/r/microsoft/windows-insider/">https://hub.docker.com/r/microsoft/windows-insider/</a></p>
<p><strong>Update:</strong> I found two of the three images in the description on Docker Hub here:</p>
<p><a href="https://hub.docker.com/r/microsoft/nanoserver/">https://hub.docker.com/r/microsoft/nanoserver/</a><br>
<a href="https://hub.docker.com/r/microsoft/windowsservercore/">https://hub.docker.com/r/microsoft/windowsservercore/</a><br>
<a href="https://hub.docker.com/r/microsoft/windows/">https://hub.docker.com/r/microsoft/windows/</a>  (still 404)</p>
<pre><code>docker pull mcr.microsoft.com/windows/nanoserver:1809
docker pull mcr.microsoft.com/windows/servercore:ltsc2019
</code></pre>
<p>I'll update the blog post when I found the image name for the full Windows image.</p>
<h2 id="portsbindtolocalhost">Ports bind to localhost</h2>
<p>When you bind a container port to a host port your containerized application can be accessed with localhost from the host. This is what we are used with Linux containers since the beginning and now you no longer have to find out the container IP address to access your application.</p>
<pre><code>docker run -p 80:80 mcr.microsoft.com/iis
start http://localhost:80
</code></pre>
<h2 id="ingressnetworking">Ingress networking</h2>
<p>I have tried one of the latest Insider builds to create a <strong>Docker Swarm</strong> with multiple Windows machines. And I'm happy to see that you can create a Windows-only Docker Swarm with manager nodes and worker nodes.<br>
I was also able to access the published port of a web server running in Windows containers.</p>
<p>After scaling that service up to have it running multiple times spread over all swarm nodes I also could see the load-balancing is working in such a Windows-only cluster.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/10/Bildschirmfoto-2018-10-03-um-16.44.58.png" alt="Service running in Windows-only docker swarm"></p>
<h2 id="namedpipesinwindowscontainers">Named pipes in Windows containers</h2>
<p>Another nice improvement is that you can bind Windows named pipes from the host into Windows containers.</p>
<p>Some tools like Traefik, Portainer UI or another Docker client want to access the Docker API and we know we could bind the Unix socket into Linux containers. With Windows Server 2019 it is possible to do the same with the Docker named pipe.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/10/Bildschirmfoto-2018-10-03-um-16.48.07.png" alt="Portainer and local Docker named pipe"></p>
<h2 id="lcow">LCOW?</h2>
<p>I could not find any announcement at MS Ignite about Linux Containers on Windows.</p>
<p>So I tried the steps to manually install the LinuxKit kernel and updated to the latest Docker EE 18.03.1-ee-3 version.</p>
<p>In this combination LCOW really works, but I cannot say how stable it is and if it's officially supported. Probably we have to wait a little longer to see a better experience to install this feature.</p>
<h2 id="getitnow">Get it now</h2>
<p>Go and get in touch with the new Windows Server 2019.</p>
<ul>
<li>Spin up a Windows Server 2019 from the Azure Marketplace.</li>
<li>Go to your MSDN subscription and download the ISO.</li>
<li>Go to the Evaluation Center <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2019">https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2019</a> and download the VHD or ISO.</li>
<li>Pick my Vagrant box from <a href="https://app.vagrantup.com/StefanScherer/boxes/windows_2019">Vagrant Cloud</a> suitable for VMware Workstation/Fusion, Hyper-V and VirtualBox.</li>
<li>Watch the videos from <a href="https://myignite.techcommunity.microsoft.com/videos?q=windows%2520container&amp;t=%257B%2522from%2522%253A%25222018-09-23T08%253A00%253A00-04%253A00%2522%252C%2522to%2522%253A%25222018-09-28T19%253A00%253A00-04%253A00%2522%257D#ignite-html-anchor">last weeks MS Ignite</a> about Windows Server 2019 and Windows Containers.</li>
<li>Go to <a href="https://chocolateyfest.com">Chocolatey Fest 2018</a> next week in San Francisco where you can attend a hands-on workshop. Use the code <strong>Speaker50</strong> when you register.</li>
</ul>
<h2 id="tldr">TL/DR</h2>
<p>When you work with Windows Containers I recommand to switch over to the new Windows Server 2019 release. It is much simpler now to work with Windows containers. With the smaller images you can deploy your application even faster.</p>
<p>You still can run your old containers from 2016 in Hyper-V isolation mode, but I recommend to rebuild them with the new Windows base images to experience faster downloads and start times.</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to use AppVeyor to build a multi-arch Docker image for Linux and Windows]]></title><description><![CDATA[<div class="kg-card-markdown"><p>After some months of private beta AppVeyor recently has announced general availability of their Linux build agents. In this blog post I want to show you what we can do with this new feature.</p>
<p>In <a href="https://stefanscherer.github.io/fork-appveyor-buildpipeline/">my previous blog post</a> I showed how you can fork the example repo and build</p></div>]]></description><link>https://stefanscherer.github.io/use-appveyor-to-build-multi-arch-docker-image/</link><guid isPermaLink="false">5b12c82dd71f9600016d99ed</guid><category><![CDATA[Docker]]></category><category><![CDATA[Docker Hub]]></category><category><![CDATA[AppVeyor]]></category><category><![CDATA[multi-arch]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sat, 02 Jun 2018 18:13:19 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>After some months of private beta AppVeyor recently has announced general availability of their Linux build agents. In this blog post I want to show you what we can do with this new feature.</p>
<p>In <a href="https://stefanscherer.github.io/fork-appveyor-buildpipeline/">my previous blog post</a> I showed how you can fork the example repo and build it your own, adjust it and learn all the details of the application, the Dockerfiles and the build steps.</p>
<p>This blog post shows the details about a Linux and Windows builds and how you can combine that to a multi-arch Docker image.</p>
<h1 id="whatisappveyor">What is AppVeyor?</h1>
<p>But first we have to start with AppVeyor. The GitHub market place shows a lot of offerings for <a href="https://github.com/marketplace/category/continuous-integration">continuous integration</a>. This is what you normally want to have automatic tests for each Git commit or pull request you receive.</p>
<p>AppVeyor is my #1 place to go if I want Windows builds. I use it for several years now, you can do your .NET builds, native C/C++ builds and even Windows Containers with it. It is really easy to attach it to your GitHub repo with a YAML file.</p>
<h1 id="whoami">Whoami</h1>
<p>After the announcement for the new Linux build agents I looked into my sample <a href="https://github.com/StefanScherer/whoami">whoami repo</a> that builds a multi-arch Docker image that works both for Linux and Windows. I was curious to find out how the Linux builds work on AppVeyor. Because then I can just use one CI provider instead of two different.</p>
<p>The CI pipeline before that evening looked like this.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/pipeline-old.png" alt="pipeline-old"></p>
<p>I used <a href="https://travis-ci.org">Travis CI</a> for all the Linux builds. There was a build matrix to build Linux Docker images for three different CPU architectures: x64, arm and arm64.</p>
<p>For the Windows builds I already used <a href="https://www.appveyor.com">AppVeyor</a> as they provider Docker builds as well.</p>
<p>The difficult part was to synchronise all builds to run the final step to create a Docker manifest that combines all Docker images to just one manifest.</p>
<h1 id="twoyamls">Two YAMLs</h1>
<p>I opened the two YAML files that describe the CI pipeline for each service:</p>
<ul>
<li><code>appveyor.yml</code> for Windows on the left side</li>
<li><code>.travis.yml</code> for Linux on the right side</li>
</ul>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/migrate-travis-to-appveyor-01.png" alt="migrate-travis-to-appveyor-01"></p>
<p>The YAML have a similar structure. There are three steps</p>
<ul>
<li>build</li>
<li>test</li>
<li>deploy (if it's a tagged release build)</li>
</ul>
<p>And the Travis build has a build matrix for three variants.</p>
<p>I started to draft the updated <code>appveyor.yml</code> how it could look like when the Linux build gets migrated from the <code>.travis.yml</code> into it.</p>
<h1 id="powershellandbashmixture">PowerShell and BASH mixture</h1>
<p>The first idea was to just re-use the Windows PowerShell scripts and the Linux BASH scripts and call in from one YAML.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/migrate-travis-to-appveyor-ps-and-sh.png" alt="migrate-travis-to-appveyor-ps-and-sh"></p>
<p>Hm, now the <code>appveyor.yml</code> looked messy. You can tell with <code>ps:</code> that you want to run PowerShell, with <code>sh:</code> you can choose BASH.</p>
<p>With the environment variable <code>APPVEYOR_YML_DISABLE_PS_LINUX: true</code> you can turn off PowerShell support for Linux.</p>
<p>But it really looked ugly.</p>
<h1 id="powershellonlinuxreally">PowerShell on Linux, really?</h1>
<p>Microsoft has announced PowerShell support on Linux months ago. But I only smiled upto now. What should I do with just another script language on Linux, I thought? It only made sense when you come from Windows and don't want to learn BASH.</p>
<p>But looking at this mixed YAML mixture I thought: &quot;Hey, let's try PowerShell on Linux here&quot; to have a platform independent script.</p>
<p>I just edited the YAML file how it should look like.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/migrate-travis-to-appveyor-only-ps.png" alt="migrate-travis-to-appveyor-only-ps"></p>
<p>Much cleaner. Oh, what about these Unix slashes? But cool, they really work in PowerShell, even on Windows.</p>
<p>The only tricky part was integrating the Travis build matrix into the AppVeyor build matrix. My use-case is running one Windows build, but three Linux builds configured by an environment variable.</p>
<p>With some excludes (thanks to AppVeyor support) the YAML now looks like this</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/migrate-travsi-to-appveyor-excludes.png" alt="migrate-travsi-to-appveyor-excludes"></p>
<p>And hey, the build matrix in AppVeyor looked promising.</p>
<ul>
<li>Windows, amd64</li>
<li>Linux, arm</li>
<li>Linux, arm64</li>
<li>Linux, amd64</li>
</ul>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-matrix-build.png" alt="appveyor-matrix-build"></p>
<p>The updated AppVeyor only CI pipeline now looks like this.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-pipeline.png" alt="appveyor-pipeline"></p>
<p>The three Windows images are done in a different way. Once there are different Docker build agents to support 1709 and 1803 images I can move that to the build matrix as well.</p>
<h1 id="appveyoryml">appveyor.yml</h1>
<p>This is the <code>appveyor.yml</code> to define a matrix build for three Linux builds and one Windows build.</p>
<pre><code>version: 1.0.{build}
image:
- Visual Studio 2017
- Ubuntu

environment:
  matrix:
    - ARCH: arm
    - ARCH: arm64
    - ARCH: amd64

matrix:
  exclude:
    - image: Visual Studio 2017
      ARCH: arm
    - image: Visual Studio 2017
      ARCH: arm64

build_script:
  - ps: ./build.ps1

test_script:
  - ps: ./test.ps1

deploy_script:
  - ps: ./deploy.ps1
</code></pre>
<h1 id="buildps1">build.ps1</h1>
<p>The platform independent build script has the <code>docker build</code> command. As the <code>Dockerfile</code> differs for Windows I have to choose a different name as well add the build argument for the Linux build. But with the <code>$isWindows</code> variable you can easily check whether this script runs in the Windows agent or the Linux agent.</p>
<pre><code>$ErrorActionPreference = 'Stop';
Write-Host Starting build

if ($isWindows) {
  docker build --pull -t whoami -f Dockerfile.windows .
} else {
  docker build -t whoami --build-arg &quot;arch=$env:ARCH&quot; .
}

docker images
</code></pre>
<h1 id="testps1">test.ps1</h1>
<p>The platform independent test script skips the ARM images, I haven't tested QEMU in the Linux builder that could help to even run the ARM images in the x64 Linux build agent.</p>
<p>The test starts the container. We could add a Invoke-WebRequest call to check if the web server responds with 200 OK. But this test is enough for now.</p>
<pre><code>Write-Host Starting test

if ($env:ARCH -ne &quot;amd64&quot;) {
  Write-Host &quot;Arch $env:ARCH detected. Skip testing.&quot;
  exit 0
}

$ErrorActionPreference = 'SilentlyContinue';
docker kill whoamitest
docker rm -f whoamitest

$ErrorActionPreference = 'Stop';
Write-Host Starting container
docker run --name whoamitest -p 8080:8080 -d whoami
Start-Sleep 10

docker logs whoamitest

$ErrorActionPreference = 'SilentlyContinue';
docker kill whoamitest
docker rm -f whoamitest
</code></pre>
<h1 id="deployps1">deploy.ps1</h1>
<p>The platform independent deploy script first pushes each platform specific image from each build agent.</p>
<p>The last build agent in the matrix, it's the Linux amd64 variant, then creates the manifest list and also pushes the manifest list to Docker Hub.</p>
<p>It first stops if there is no tagged build. So only GitHub releases will be pushed to Docker Hub.</p>
<pre><code>$ErrorActionPreference = 'Stop';

if (! (Test-Path Env:\APPVEYOR_REPO_TAG_NAME)) {
  Write-Host &quot;No version tag detected. Skip publishing.&quot;
  exit 0
}
</code></pre>
<p>Then we define the Docker image name for the final Docker image (the manifest list, to be exact):</p>
<pre><code>$image = &quot;stefanscherer/whoami&quot;

Write-Host Starting deploy
</code></pre>
<h1 id="experimentalbatteriesincluded">(Experimental) batteries included</h1>
<p>To create the manifest list I use the Docker CLI to avoid downloading extra tools. But we have to enable experimental features in Docker CLI first:</p>
<pre><code>if (!(Test-Path ~/.docker)) { mkdir ~/.docker }
'{ &quot;experimental&quot;: &quot;enabled&quot; }' | Out-File ~/.docker/config.json -Encoding Ascii
</code></pre>
<p>I showed these experimental feature <a href="https://www.slideshare.net/stefscherer/azure-meetup-stuttgart-multiarch-docker-images">in several talks</a>. But here is a small overview. In addition to <code>docker push</code> - or <code>docker image push</code> there are two new commands: <code>docker manifest create</code> and <code>docker manifest push</code>:</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/docker-manifest-create-push.png" alt="docker-manifest-create-push"></p>
<p>For the next steps we need to be logged in with the Docker Hub account.</p>
<pre><code>docker login -u=&quot;$env:DOCKER_USER&quot; -p=&quot;$env:DOCKER_PASS&quot;
</code></pre>
<h1 id="pushtheplatformspecificimage">Push the platform specific image</h1>
<p>Now the script tags and pushes the platform specific Docker image with a correpsonding tag name.</p>
<pre><code>$os = If ($isWindows) {&quot;windows&quot;} Else {&quot;linux&quot;}
docker tag whoami &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME&quot;
docker push &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME&quot;
</code></pre>
<h1 id="windowsbuildrebasedockerimage">Windows build: rebase-docker-image</h1>
<p>For the Windows build I additionally run my <code>rebase-docker-image</code> tool. This is a hacker tool to replace the Windows base image from a given image with another version of the Windows base image. This works only in a few cases, but the whoami Golang binary and Dockerfile is safe for such hacks as this app really doesn't depend on the specific underlying base image. You can read more about that tool in my blog post <a href="https://stefanscherer.github.io/poc-build-images-for-1709-without-1709/">PoC: How to build images for 1709 without 1709</a>.</p>
<p>We create both a 1709 and 1803 variant as long as there is no AppVeyor build agent that is able to produce 'native' Docker builds for that.</p>
<pre><code>if ($isWindows) {
  # Windows
  Write-Host &quot;Rebasing image to produce 1709 variant&quot;
  npm install -g rebase-docker-image
  rebase-docker-image `
    &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME&quot; `
    -t &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME-1709&quot; `
    -b microsoft/nanoserver:1709

  Write-Host &quot;Rebasing image to produce 1803 variant&quot;
  npm install -g rebase-docker-image
  rebase-docker-image `
    &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME&quot; `
    -t &quot;$($image):$os-$env:ARCH-$env:APPVEYOR_REPO_TAG_NAME-1803&quot; `
    -b microsoft/nanoserver:1803

}
</code></pre>
<h1 id="linuxbuildcreateandpushmanifestlist">Linux build: Create and push manifest list</h1>
<p>The Linux amd64 build agent runs as the last one in the matrix build, so it's easy to create the manifest list. All platform specific Docker images are already pushed to Docker Hub.</p>
<p>We run <code>docker manifest create</code> and then <code>docker manifest push</code> for the target image name.</p>
<pre><code>else {
  # Linux
  if ($env:ARCH -eq &quot;amd64&quot;) {
    # The last in the build matrix
    docker -D manifest create &quot;$($image):$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):linux-amd64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):linux-arm-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):linux-arm64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME-1709&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME-1803&quot;
    docker manifest annotate &quot;$($image):$env:APPVEYOR_REPO_TAG_NAME&quot; &quot;$($image):linux-arm-$env:APPVEYOR_REPO_TAG_NAME&quot; --os linux --arch arm --variant v6
    docker manifest annotate &quot;$($image):$env:APPVEYOR_REPO_TAG_NAME&quot; &quot;$($image):linux-arm64-$env:APPVEYOR_REPO_TAG_NAME&quot; --os linux --arch arm64 --variant v8
    docker manifest push &quot;$($image):$env:APPVEYOR_REPO_TAG_NAME&quot;

    Write-Host &quot;Pushing manifest $($image):latest&quot;
    docker -D manifest create &quot;$($image):latest&quot; `
      &quot;$($image):linux-amd64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):linux-arm-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):linux-arm64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME-1709&quot; `
      &quot;$($image):windows-amd64-$env:APPVEYOR_REPO_TAG_NAME-1803&quot;
    docker manifest annotate &quot;$($image):latest&quot; &quot;$($image):linux-arm-$env:APPVEYOR_REPO_TAG_NAME&quot; --os linux --arch arm --variant v6
    docker manifest annotate &quot;$($image):latest&quot; &quot;$($image):linux-arm64-$env:APPVEYOR_REPO_TAG_NAME&quot; --os linux --arch arm64 --variant v8
    docker manifest push &quot;$($image):latest&quot;
  }
}
</code></pre>
<h1 id="checktheresultingimage">Check the resulting image</h1>
<p>With the Docker image <code>mplatform/mquery</code> from Docker Captain Phil Estes you can inspect such multi-arch images.</p>
<pre><code>$ docker run --rm mplatform/mquery stefanscherer/whoami
Image: stefanscherer/whoami
 * Manifest List: Yes
 * Supported platforms:
   - linux/amd64
   - linux/arm/v6
   - linux/arm64/v8
   - windows/amd64:10.0.14393.2248
   - windows/amd64:10.0.16299.431
   - windows/amd64:10.0.17134.48
</code></pre>
<p>As you can see this image provides three Linux and three Windows variants. Windows can choose the best fit to the Windows kernel version to avoid running Windows Containers in Hyper-V mode.</p>
<p>Now try this image on any platform with</p>
<pre><code>docker run -d -p 8080:8080 stefanscherer/whoami
</code></pre>
<p>It will work on your Raspberry Pi, running <a href="https://blog.hypriot.com">HypriotOS</a> or manually installed Docker. It will work on any Linux cloud machine, it will work in Docker for Mac or Docker 4 Windows.</p>
<p>Then access the published port 8080 with a browser. You will see that it shows the container name and the OS and CPU architecture name of the compiled binary.</p>
<p>If you have a use-case for such a multi-arch / multi-os image and want to provide it to your community, <a href="https://github.com/StefanScherer/whoami">fork my GitHub repo</a> and also <a href="https://stefanscherer.github.io/fork-appveyor-buildpipeline/">fork the AppVeyor build pipeline</a>. It's really easy to get started.</p>
<p>I hope you enjoyed this blog post and I would be happy if you share it with your friends. I'm <a href="https://twitter.com/stefscherer">@stefscherer</a> on Twitter.</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to build a forked GitHub repo: Spot the YAML]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Maybe you find an interesting project on GitHub and want to build it your own. How can you do that? Maybe the project is written in a programming language that you are not familiar with. Or it uses a lot of tools to build that you don't have locally. Of</p></div>]]></description><link>https://stefanscherer.github.io/fork-appveyor-buildpipeline/</link><guid isPermaLink="false">5b12b403d71f9600016d99e8</guid><category><![CDATA[AppVeyor]]></category><category><![CDATA[GitHub]]></category><category><![CDATA[Docker Hub]]></category><category><![CDATA[Docker]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sat, 02 Jun 2018 16:45:00 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Maybe you find an interesting project on GitHub and want to build it your own. How can you do that? Maybe the project is written in a programming language that you are not familiar with. Or it uses a lot of tools to build that you don't have locally. Of course you have hear of Docker to put all build tools and dependencies into a container. But what if the project doesn't provide a Dockerfile?</p>
<p>Sometimes it is easier to just look at the repo. Does it have some green build badges in the <a href="http://README.md">README.md</a>? That is a good first hint that they use a CI pipeline. Look for YAML files that show you which CI service the project uses.</p>
<p>I'll show you an example with one of my GitHub repos. This project builds a Docker image with a simple web server, it's written in Golang, bla bla bla...<br>
The point is, there is a CI pipeline for AppVeyor and the corresponding YAML file also is in my repo. Let's have a look how you can fork my repo and fork the build pipeline.</p>
<h2 id="whatisappveyor">What is AppVeyor?</h2>
<p>The GitHub market place shows a lot of offerings for <a href="https://github.com/marketplace/category/continuous-integration">continuous integration</a>. This is what you normally want to have automatic tests for each Git commit or pull request you receive.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor.png" alt="appveyor"></p>
<p><a href="https://www.appveyor.com">AppVeyor</a> is my #1 place to go if I want Windows builds. It is really easy to attach it to your GitHub repo with a YAML file.</p>
<p>It can be as simple as this example <code>appveyor.yml</code> file.</p>
<pre><code>version: 1.0.{build}
image:
- Visual Studio 2017

build_script:
  - ps: ./build.ps1

test_script:
  - ps: ./test.ps1

deploy_script:
  - ps: ./deploy.ps1
</code></pre>
<h2 id="forkingbuildpipelines">Forking build pipelines</h2>
<p>What is the advantage to write a YAML file you may ask. Well I really like to share not only my code, but also my pipeline with the community. Others can fork my repo and only need a few clicks to attach the fork and have the complete pipeline up and running for themselves.</p>
<h1 id="whoamiexample">Whoami example</h1>
<p>In the next screenshots I will show you how easy it is to setup a build pipeline for a repo that you have seen the first time.</p>
<p>Go to the GitHub repo <a href="https://github.com/StefanScherer/whoami">https://github.com/StefanScherer/whoami</a>.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-whoami-repo.png" alt="GitHub StefanScherer/whoami repo"></p>
<p>You can fork it to your own GitHub account with the &quot;Fork&quot; button. GitHub will prepare the fork for you.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-forking-repo.png" alt="github forking repo"></p>
<p>Now scroll down to the <a href="http://README.md">README.md</a>. The next thing is to attach the pipeline to your fork.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-readme-link-to-appveyor.png" alt="github repo, go to readme and follow link to appveyor"></p>
<p>Just click on the the AppVeyor build badge to jump to the AppVeyor site, maybe open a new tab as we need the GitHub site later as well.<br>
Now you can see the build status of my repo. This is not your fork yet, but we now can sign in to AppVeyor.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-buildstatus-upstream-repo.png" alt="appveyor buildstatus upstream repo, link to sign in"></p>
<p>Click on &quot;SIGN IN&quot; in the top right corner. AppVeyor will ask you how to sign in. Just use GitHub.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-sign-in.png" alt="appveyor sign in with github"></p>
<p>Now GitHub will ask you if you want to give AppVeyor read-only access to your user data and public teams.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-auth-login.png" alt="appveyor-auth-login"></p>
<p>After that you have connected AppVeyor to your account.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-lets-start.png" alt="appveyor-lets-start"></p>
<p>Now this has to be done only once. After that you can add the forked repo to build on AppVeyor. Click on &quot;NEW PROJECT&quot; in the top left corner.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-select-github-repo.png" alt="appveyor-select-github-repo"></p>
<p>You can choose between several version control systems. As you have forked a GitHub repo, click on &quot;GitHub&quot; and then on &quot;Authorize GitHub&quot;.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-auth-github-repos.png" alt="appveyor-auth-github-repos"></p>
<p>AppVeyor needs some more access rights to add the Web hook for you and to send commit statuses. Click on &quot;Authorize appveyor&quot; to grant access.</p>
<p>Now you will see a list of GitHub repos of your GitHub account.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-add-github-project.png" alt="appveyor-add-github-project"></p>
<p>Move to the &quot;whoami&quot; repo and click on the <strong>&quot;+ Add&quot;</strong> button on the right side. The UI isn't the best here, I often missed the Add link for my first projects.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-github-repo-connected.png" alt="appveyor-github-repo-connected"></p>
<p>Congratulations! You have the build pipeline up and running. No VM's to setup, no installation required. You didn't have to clone the repo to your local machine yet.</p>
<p>Each Git commit will now trigger a build on AppVeyor with the <code>appveyor.yml</code> file that comes with the sources in the GitHub repo. You don't have to think what steps you have to do to build this project.</p>
<h2 id="adjustthefork">Adjust the fork</h2>
<h3 id="adjustreadmemd">Adjust <a href="http://README.md">README.md</a></h3>
<p>The first change should be to adjust the build badge in the <code>README.md</code> to link to your forked build.</p>
<p>Let's do that in the browser, so you still don't have to clone the repo to you local machine.</p>
<p>But first we have to grab the build badge link. Go to &quot;Settings&quot; and then to &quot;Badges&quot;. You will see some samples, pick the Sample Markdown code</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-settings-badge-markdown.png" alt="appveyor-settings-badge-markdown"></p>
<p>Now head over to the GitHub browser tab and edit the <code>README.md</code> file.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-edit-readme.png" alt="github-edit-readme"></p>
<p>In this editor paste the new build badge link. Also adjust the Docker Hub badge to point to your desired Docker Hub image name. After that scroll down and commit the changes.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-commit-readme-changes.png" alt="github-commit-readme-changes"></p>
<p>Head back to AppVeyor and you will see your first build running.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-first-build-running.png" alt="appveyor-first-build-running"></p>
<p>Isn't that fantastic? You just triggered a build from your browser. You can follow the build (it's a matrix build, we will have a closer look <a href="https://stefanscherer.github.io/use-appveyor-to-build-multi-arch-docker-image/">in the next blog post</a>).</p>
<p>After a while the build is green.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/Bildschirmfoto-2018-06-02-um-17.42.57.png" alt="Bildschirmfoto-2018-06-02-um-17.42.57"></p>
<h3 id="adjustdeployps1">Adjust deploy.ps1</h3>
<p>The second change in the forked repo is to adjust the Docker image name to deploy it to Docker Hub for when you start a GitHub release build.</p>
<p>Head over to GitHub browser tab and edit the <code>deploy.ps1</code> script.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-edit-deploy-ps1.png" alt="github-edit-deploy-ps1"></p>
<p>In line 8 you have to adjust the <code>$image</code> variable to fit your needs.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-commit-changes-deploy-ps1.png" alt="github-commit-changes-deploy-ps1"></p>
<p>After that commit the changes, a second build will be triggered. But nothing more happens in the second build.</p>
<h2 id="tellmeasecret">Tell me a secret</h2>
<p>The <code>appveyor.yml</code> is configured to deploy the Docker image only during a release build. For such releases AppVeyor needs access to your Docker registry you want to push to. In our case it's the Docker Hub.</p>
<p>This is done with secret environment variables. You can either use secrets in the <code>appveyor.yml</code> or just edit the environment variables in the AppVeyor browser tab. I'll show you the latter how to do it.</p>
<p>Go to &quot;SETTINGS&quot; and click the &quot;Environment&quot; tab. We need to add two environment variables</p>
<ul>
<li>DOCKER_USER</li>
<li>DOCKER_PASS</li>
</ul>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-project-settings-environment.png" alt="appveyor-project-settings-environment"></p>
<p>Then scroll down and <strong>click on &quot;Save&quot;</strong>. This is the second thing that could be improved in the UI. You often don't see this &quot;Save&quot; button.</p>
<p>If you don't like to add your real Docker Hub account a good practise is to use another Docker Hub account for just the pushes and grant that account write access to only the Docker Hub images you want to.</p>
<h2 id="releaseit">Release it!</h2>
<p>Now, the build pipeline is set up in AppVeyor, as you have seen, the build and minimal tests were green. Now it's time to release the first Docker image.</p>
<p>Go to your forked GitHub repo again. There is a link to the &quot;releases&quot;. Click on &quot;releases&quot;.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-go-to-releases.png" alt="github-go-to-releases"></p>
<p>You have forked all the tags, but not the releases. Now let's &quot;Draft a new release&quot; to trigger a build.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-releases-draft-new-release.png" alt="github-releases-draft-new-release"></p>
<p>Use for example &quot;2.0.0&quot; as new release and tag name, enter some useful description.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-draft-release-2.png" alt="github-draft-release-2"></p>
<p>Then press &quot;Publish release&quot;. This also triggers a new build in AppVeyor, this time a tagged build.</p>
<p>In AppVeyor you can see the tag name &quot;2.0.0&quot;<br>
<img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-release-build.png" alt="appveyor-release-build"></p>
<p>You now also can follow the build, but I'll explain it in more detail <a href="https://stefanscherer.github.io/use-appveyor-to-build-multi-arch-docker-image/">in the next blog post</a>. After some minutes the build is completed and green.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/appveyor-release-build-finished-1.png" alt="appveyor-release-build-finished-1"></p>
<p>Now, do we really have a Docker image pushed to Docker Hub? Let's check. Go back to your GitHub repo and check if the Docker Hub badge also works.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-dockerhub-badges.png" alt="github-dockerhub-badges"></p>
<p>And yes, there it is. You have successfully published a Docker image from an application you don't really have to understand the language or how to setup the build steps for that.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/dockerhub-whoami.png" alt="dockerhub-whoami"></p>
<p>That's the &quot;let me do it&quot; first approach. Now you have time to look at all the files. Start with the <code>appveyor.yml</code>, the YAML is the start of the build pipeline.</p>
<p>Or start with the application code which is written in Golang.</p>
<h1 id="tldrshareyouryaml">TL/DR: Share your YAML</h1>
<p>In this blog post you have seen how important it is to share not only the code, but also the build pipeline. You have learned to watch out for YAML files. There are other CI services out there, but the pattern is almost the same. Look for <code>.travis.yml</code>, <code>.circleci/config.yml</code> and similar names.</p>
<p>If you liked this blog post please share it with your friends. You can follow me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a> as well.</p>
</div>]]></content:encoded></item><item><title><![CDATA[Ship happens. Secrets leaked to GitHub, what next?]]></title><description><![CDATA[<div class="kg-card-markdown"><p>What a wonderful day. I just changed some code in one of my weekend projects and then it happened. I totally screwed it up, I accidentally pushed some secrets to a GitHub pull request. Yes, ship happens. We're all humans and make mistakes. We normally blog about success, but I</p></div>]]></description><link>https://stefanscherer.github.io/ship-happens-secrets-leaked-to-github/</link><guid isPermaLink="false">5b1288ffd71f9600016d99e0</guid><category><![CDATA[GitHub]]></category><category><![CDATA[secrets]]></category><category><![CDATA[ubiquity]]></category><category><![CDATA[unify]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sat, 02 Jun 2018 13:15:26 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>What a wonderful day. I just changed some code in one of my weekend projects and then it happened. I totally screwed it up, I accidentally pushed some secrets to a GitHub pull request. Yes, ship happens. We're all humans and make mistakes. We normally blog about success, but I use my mistake to talk about how to fix this and how to prevent it from happening in the future again.</p>
<h1 id="thebadmistake">The bad mistake</h1>
<p>Well, I edited some code of my flash script to flash Raspberry Pi SD cards. This tool can also inject configuration to boot your Pi without any manual interaction to a specified hostname, or add your WiFi settings so it can join your wireless network automatically.</p>
<p>I pushed some code to a work-in-progress pull request when I saw my mistake on GitHub:</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/my-pull-request.png" alt="OMG, my pull request shows my WiFi PSK"></p>
<p>WTF, how did I ... ?</p>
<p>Well, for convenience reasons I kept a configuration file in the repo to easily flash a new SD card image with WiFi settings. And I can't really remember, but I eventually typed <code>git add .</code> and <code>git push</code> some minutes ago without recognising that this was a really, really bad idea.</p>
<h1 id="panicwhatnext">Panic, what next?</h1>
<p>I immediatelly went to my Ubiquity Cloud controller and changed the Wireless Network Security Key.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/unify2.png" alt="Unify Wireless Network settings"></p>
<p>But that was the next mistake. OK, I've changed the security key. But after a moment I realized that I also have some unattended boxes lying around in my house that use the old key to connect to my WiFi. My AirPort Express boxes for example are connected wirelessly.</p>
<h2 id="calmdownthinkandorganize">Calm down, think, and organize</h2>
<p>OK, changing the Security Key as first step is probably not the best idea. I don't want to run to each box with a patch cable to reconfigure it. Instead I've changed the key back to the old, compromised one and reconfigured all my wireless devices that I can reach through WiFi.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/airport1.png" alt="AirPort devices"></p>
<h2 id="reconfigurewirelessdevicesfirst">Reconfigure wireless devices first</h2>
<p>The devices with the dotted lines are connected through WiFi. Edit the wireless network password with the AirPort app on your Mac.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/airport2.png" alt="Change AirPort wireless network password"></p>
<p>After that change they will drop out of WiFi as they now have the new, but not actually working password.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/airport-unreachable.png" alt="AirPort devices unreachable"></p>
<p>Repeat that for all devices and think of other wireless devices that you can update without climbing up ladders or other hidden places.</p>
<h2 id="changeyourwifisecuritykey">Change your WiFi security key</h2>
<p>After that I changed the wireless security key in the Unify cloud controller. Save the new WiFi key in your password manager, I use 1Password</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/uniqif-change-security-key2.png" alt="Change wireless security key in Unify cloud controller"></p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/1password-1.png" alt="Update key in 1Password password manager"></p>
<p>After reconnecting to the new and now secure WiFi with the updated key I thought of the next steps. OK, the whole family has to update their smartphones and tables to connect to the WiFi again. That is managable. Now I'm coming to the next phase.</p>
<h1 id="cleaningup">Cleaning up</h1>
<p>The next steps was to clean up the pull request to get rid of the accidentally added files. You might think when you are quick nobody has seen your change and you can skip changing your WiFi secret at all. I'll prove you wrong in the next few minutes.</p>
<p>First I commented on my mistake to laugh at it, that's just relieving.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-pr-leak.png" alt="Comment your mistake on GitHub"></p>
<h2 id="removefilesremovecommits">Remove files, remove commits</h2>
<p>Now it's time to clean up the pull request branch and remove the unwanted files. We could just do <code>git rm wifi.yml</code>, but this will be added as a new commit to Git. Git has a history of each commit. I also want to get rid of these old commits.</p>
<p>These were my steps to cleanup the pull request branch.</p>
<p>I first squashed the pull request to one commit.</p>
<pre><code>git rebase -i $(git merge-base $(git rev-parse --abbrev-ref HEAD) master)
</code></pre>
<p>Then in the editor just <code>pick</code> the first commit and change all other commits to <code>squash</code>.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/git-squash.png" alt="git squash"></p>
<p>Now I have only one commit. This commit can be easily undone with</p>
<pre><code>git reset HEAD~1
</code></pre>
<p>Then add your secret files to the <code>.gitignore</code> file and add everything and commit it again.</p>
<p>Now your local pull request branch has only the files wanted in one single commit. But GitHub still stores the secret files. With the next command we'll fix that.</p>
<h2 id="gitpushf">git push -f</h2>
<p>When things went bad sometimes a <code>git push -f</code> is needed. But beware: This will overwrite the history in your Git repo. You really have to know what are you doing here. Don't use <code>git push -f</code> in a panic. Calm down first. Otherwise you will make it even worse.</p>
<p>But to remove the unwanted commits you need to overwrite the pull request branch.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/git-push-f-pr.png" alt="Dangerous: git push -f"></p>
<pre><code>git push -f origin add-version 
</code></pre>
<h2 id="everythingconcealednoway">Everything concealed? No way</h2>
<p>When you now look at the GitHub pull request you might think that every secret vanished and it's safe to keep the old WiFi password. No, GitHub has an incredible database, don't think that that this information was removed.</p>
<p>Each pull request can be commented and even after a <code>git push -f</code> some of the comments got outdated on source that no longer exist. But this is still visible and retrievable.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-show-outdated-1.png" alt="GitHub show outdated comments"></p>
<p>Look closer, there is a &quot;Show outdated&quot; link. You can open this</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/github-show-outdated-2.png" alt="GitHub show outdated comments still visible"></p>
<p>So whenever such a data breach happens, be prepared to change your secrets. <strong>If it hurts, do it more often.</strong></p>
<h1 id="understandingthecause">Understanding the cause</h1>
<p>After all this disaster recovery and cleanup there is still something to learn. What was the root cause and how can I prevent to make the same mistake again?</p>
<h2 id="gitadd">git add .</h2>
<p>The <code>git add .</code> command adds all modified and also untracked files and <code>git push</code> pushes all this code to GitHub into your already open pull request branch.<br>
Yes, I'm lazy and often commit everything as I'm normally work on one thing in a project.</p>
<p>I normally recognize such secret files from adding them, but as I realised the hard way is that you will type <code>git add .</code> at some point in a hurry without even recognizing it.</p>
<p>I scrolled up my terminal and found the situation where everything went wrong very soon.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/06/git-untracked-files.png" alt="Git repo with untracked files"></p>
<p>This is a bad smell having untracked files.</p>
<p>What can this be fixed?</p>
<ul>
<li>Learn to not use <code>git add .</code>. I don't think that will work as I'm trained to type this and it's hard to break a habit.</li>
<li>Maybe prevent <code>git add .</code>?, see <a href="https://stackoverflow.com/questions/25884007/disable-git-add-command">Stack Overflow</a> I'm not going this hard way.</li>
<li>Don't leave untracked files in your repo, yeah that sounds better.</li>
<li>Add local secret files to your <code>.gitignore</code> file. So a <code>git add .</code> is harmless.</li>
<li>Don't create such local secret files at all. Well you want something automated and just need config files with secrets.</li>
<li>Create local files without the secrets and inject secrets on the fly. That also sounds like a good plan.</li>
</ul>
<p>I'll look closer into the last idea to inject secrets on the fly. Don't leave secrets unprotected on your harddrive. Use command line interfaces for your password managers.</p>
<ul>
<li>Use <code>pass</code> - <a href="https://www.passwordstore.org">Pass: The Standard Unix Password Manager</a> that keeps secrets in GPG encrypted files which are also under version control in a separate Git repo.</li>
<li>I'll also have a look at the <a href="https://stefanscherer.github.io/ship-happens-secrets-leaked-to-github/(https://support.1password.com/command-line/)">1Password command line tool</a> <code>op</code>.</li>
</ul>
<p>You cannot change the past, you only can learn to make it better in the future.</p>
<p>I hope you find this blog post useful and I love to hear your feedback and experience about similar mistakes or better tips how to protect yourself from doing mistakes. Just drop a comment below or ping me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[How to find dependencies of containerized Windows apps]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Running applications in Windows containers keeps your server clean. The container image must contain all the dependencies that the application needs to run, for example all its DLL's. But sometimes it's hard to figure out why an application doesn't run in a container. Here's my way to find out what's</p></div>]]></description><link>https://stefanscherer.github.io/find-dependencies-in-windows-containers/</link><guid isPermaLink="false">5a84a3660f689f0001bafe61</guid><category><![CDATA[windows-containers]]></category><category><![CDATA[Docker]]></category><category><![CDATA[Hyper-V]]></category><category><![CDATA[process-monitor]]></category><category><![CDATA[sysinternals]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Wed, 14 Feb 2018 23:01:47 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Running applications in Windows containers keeps your server clean. The container image must contain all the dependencies that the application needs to run, for example all its DLL's. But sometimes it's hard to figure out why an application doesn't run in a container. Here's my way to find out what's missing.</p>
<h1 id="processmonitor">Process Monitor</h1>
<p>To find out what's going on in a Windows Container I often use the <a href="https://sysinternals.com">Sysinternals</a> Process Monitor. It can capture all major syscalls in Windows such as file activity, starting processes, registry and networking activity.</p>
<p>But how can we use procmon to monitor inside a Windows container?</p>
<p>Well, I heard today that you can run procmon from command line to start and stop capturing events. I tried <a href="https://github.com/StefanScherer/dockerfiles-windows/tree/master/procmon">running procmon in a Windows container</a>, but it doesn't work correctly at the moment.</p>
<p>So the next possibilty is to run procmon on the container host.</p>
<p>On Windows 10 you only have Hyper-V containers. These are &quot;black boxes&quot; from your host operating system. The Process Monitor cannot look inside Hyper-V containers.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/procmon_windows10_hyperv_container.png" alt="procmon_windows10_hyperv_container"></p>
<p>To investigate a Windows container we need the &quot;normal&quot; Windows containers without running in Hyper-V isolation. The best solution I came up with is to run a Windows Server 2016 VM and install Process Monitor inside that VM.</p>
<p>When you run a Windows container you can see the container processes in the Task Manager of the Server 2016 VM. And Process Monitor can also see what these processes are doing. We have made some containers out of &quot;glass&quot; to look inside.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/procmon_windows_container_glass.png" alt="procmon_windows_container_glass"></p>
<h1 id="examplepostgresql">Example: PostgreSQL</h1>
<p>Let's try this out and put the PostgreSQL database server into a Windows container.</p>
<p>The following <code>Dockerfile</code> downloads the ZIP file of PostgreSQL 10.2, extracts all files and removes the ZIP file again.</p>
<pre><code># escape=`
FROM microsoft/windowsservercore:10.0.14393.2007 AS download

SHELL [&quot;powershell&quot;, &quot;-Command&quot;, &quot;$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';&quot;]

ENV PG_VERSION 10.2-1

RUN Invoke-WebRequest $('https://get.enterprisedb.com/postgresql/postgresql-{0}-windows-x64-binaries.zip' -f $env:PG_VERSION) -OutFile 'postgres.zip' -UseBasicParsing ; `
    Expand-Archive postgres.zip -DestinationPath C:\ ; `
    Remove-Item postgres.zip
</code></pre>
<p>Now build and run a first container to try out the <code>postgres.exe</code> inside the container.</p>
<pre><code>docker build -t postgres .
docker run -it postgres cmd
</code></pre>
<p>Navigate into <code>C:\pgsql\bin</code> folder and run <code>postgres.exe -h</code>.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/postgres-nooutput.png" alt="postgres no output"></p>
<p>As you can see, nothing happens. No output. You just are back to the CMD prompt.</p>
<p>Now it's time to install <code>procmon.exe</code> on the container host and run it.</p>
<p>Open a PowerShell terminal in your Windows Server 2016 VM and run</p>
<pre><code>iwr -usebasicparsing https://live.sysinternals.com/procmon.exe -outfile procmon.exe
</code></pre>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/install-procmon.png-shadow.png" alt="install procmon"></p>
<p>Now run <code>procmon.exe</code> and define a filter to see only file activity looking for DLL files and start capturing.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/procmon-filter.png-shadow.png" alt="define procmon filter"></p>
<p>I have a prepared filter available for download: <a href="https://github.com/StefanScherer/dockerfiles-windows/raw/master/procmon/depends.PMF">depends.PMF</a><br>
Go to <strong>Filter</strong>, then <strong>Organize Filters...</strong> and then <strong>Import...</strong></p>
<p>Now in your container run <code>postgres.exe -h</code> again.</p>
<p>As you can see Process Monitor captures file access to <code>\Device\Harddisk\VolumeXX\psql\bin\</code> which is a folder in your container.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/procmon-postgres.png-shadow.png" alt="procmon postgres capture"></p>
<p>The interesting part is which DLL's cannot be found here. The <code>MSVCR120.dll</code> is missing, the Visual Studio Runtime DLL's.</p>
<p>For other applications you might have to look for config files or folders that are missing that stops your app from running in a Windows container.</p>
<p>Let's append the missing runtime in the <code>Dockerfile</code> with the next few lines:</p>
<pre><code>RUN Invoke-WebRequest 'http://download.microsoft.com/download/0/5/6/056DCDA9-D667-4E27-8001-8A0C6971D6B1/vcredist_x64.exe' -OutFile vcredist_x64.exe ; `
    Start-Process vcredist_x64.exe -ArgumentList '/install', '/passive', '/norestart' -NoNewWindow -Wait ; `
    Remove-Item vcredist_x64.exe
</code></pre>
<p>Build the image and run another container and see if it works now.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/postgres-usage.png" alt="postgres usage"></p>
<p>Yes, this time we see the <code>postgres.exe</code> usage, so it seems we have solved all our dependency problems.</p>
<h1 id="gonanoserver">Go NanoServer</h1>
<p>Now we have a Windows Server Core image with PostgreSQL server. The image size is now 11.1GByte. Let's go one step further and make it a much smaller NanoServer image.</p>
<p>In NanoServer we cannot run MSI packages or vcredist installers, and soon there is also no PowerShell. But with a so called <strong>multi-stage build</strong> it's easy to <code>COPY</code> deploy the PostgreSQL binaries and dependencies into a fresh NanoServer image.</p>
<p>We append some more lines to our <code>Dockerfile</code>. Most important line is the second <code>FROM</code> line to start a new stage with the smaller NanoServer image.</p>
<pre><code>FROM microsoft/nanoserver:10.0.14393.2007
</code></pre>
<p>Then we <code>COPY</code> the <code>pgsql</code> folder from the first stage into the NanoServer image, as well as the important runtime DLL's.</p>
<pre><code>COPY --from=download /pgsql /pgsql
COPY --from=download /windows/system32/msvcp120.dll /pgsql/bin/msvcp120.dll
COPY --from=download /windows/system32/msvcr120.dll /pgsql/bin/msvcr120.dll
</code></pre>
<p>Set the <code>PATH</code> variable to have all tools accessible, expose the standard port and define a command.</p>
<pre><code>RUN setx /M PATH &quot;C:\pgsql\bin;%PATH%&quot;

EXPOSE 5432
CMD [&quot;postgres&quot;]
</code></pre>
<p>Now build the image again and try it out with</p>
<pre><code>docker run postgres postgres.exe --help
</code></pre>
<p><img src="https://stefanscherer.github.io/content/images/2018/02/docker-run-postgres-nano.png" alt="docker run postgres in nano"></p>
<p>We still see the usage, so the binaries also work fine in NanoServer. The final postgres images is down at 1.64GByte.<br>
If you do this with a NanoServer 1709 or Insider image the sizes is even smaller at 738MByte. You can have a look at the compressed sizes on the Docker Hub at <a href="https://hub.docker.com/r/stefanscherer/postgres-windows/tags/">stefanscherer/postgres-windows</a>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Process Monitor can help you find issues that prevent applications to run properly in Windows containers. Run it from a Server 2016 container host to observe your or a foreign application.</p>
<p>I hope you find this blog post useful and I love to hear your feedback and experience about Windows containers. Just drop a comment below or ping me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[Terraforming a Windows Insider Server in Azure]]></title><description><![CDATA[<div class="kg-card-markdown"><p>There may be different ways to run the Windows Insider Server Preview builds in Azure. Here's my approach to run a Windows Docker engine with the latest Insider build.</p>
<h2 id="buildtheazurevm">Build the Azure VM</h2>
<p>On your local machine clone the <a href="https://github.com/StefanScherer/packer-windows">packer-windows</a> repo which has a Terraform template to build an Azure</p></div>]]></description><link>https://stefanscherer.github.io/terraforming-a-windows-insider-server-in-azure/</link><guid isPermaLink="false">5a64f348845d55000179abc4</guid><category><![CDATA[Azure]]></category><category><![CDATA[Docker]]></category><category><![CDATA[windows-containers]]></category><category><![CDATA[Insider]]></category><category><![CDATA[Windows]]></category><category><![CDATA[Terraform]]></category><category><![CDATA[Vagrant]]></category><category><![CDATA[Packer]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sun, 21 Jan 2018 21:32:41 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>There may be different ways to run the Windows Insider Server Preview builds in Azure. Here's my approach to run a Windows Docker engine with the latest Insider build.</p>
<h2 id="buildtheazurevm">Build the Azure VM</h2>
<p>On your local machine clone the <a href="https://github.com/StefanScherer/packer-windows">packer-windows</a> repo which has a Terraform template to build an Azure VM. The template chooses a V3 machine which is able to run nested VM's.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/insider_in_azure_terraform_apply.png" alt="Create a VM in Azure with Terraform"></p>
<p>You need <a href="https://terraform.io">Terraform</a> on your local machine which can be installed with a package manager.</p>
<p>Mac:</p>
<pre><code>brew install terraform
</code></pre>
<p>Windows:</p>
<pre><code>choco install terraform
</code></pre>
<p>Now clone the GitHub repo and go to the template.</p>
<pre><code>git clone https://github.com/StefanScherer/packer-windows
cd packer-windows/nested/terraform
</code></pre>
<p>Adjust the <code>variables.tf</code> file with resource group name, account name and password, region and other things. You also need some information for Terraform to create resources in your Azure account. Please read the <a href="https://www.terraform.io/docs/providers/azurerm/">Azure Provider</a> documentation for details how to obtain these values.</p>
<pre><code>export ARM_SUBSCRIPTION_ID=&quot;uuid&quot;
export ARM_CLIENT_ID=&quot;uuid&quot;
export ARM_CLIENT_SECRET=&quot;uuid&quot;
export ARM_TENANT_ID=&quot;uuid&quot;

terraform apply
</code></pre>
<p>This command will take some minutes until the VM is up and running. It also runs a provision script to install further tools for you.</p>
<h2 id="rdpintothepackerbuildervm">RDP into the Packer builder VM</h2>
<p>Now log into the Azure VM with a RDP client. This VM has Hyper-V installed as well as Packer and Vagrant, the tools we will use next.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/insider_in_azure_packer_build_vagrant_up.png" alt="Run Packer and Vagrant in Azure VM"></p>
<h2 id="buildtheinsidervm">Build the Insider VM</h2>
<p>The next step is to build the Windows Insider Server VM. We will use <a href="https://packer.io">Packer</a> for the task. This produces a Vagrant box file that can be re-used locally on a Windows 10 machine.</p>
<p>Clone the packer-windows repo and run the Packer build with the manually downloaded Insider ISO file.</p>
<pre><code>git clone https://github.com/StefanScherer/packer-windows
cd packer-windows

packer build --only=hyperv-iso --var iso_url=~/Downloads/Windows_InsiderPreview_Server_2_17074.iso windows_server_insider_docker.json
</code></pre>
<p>This command will take some minutes as it also downloads the Insider Docker images to have them cached when you start a new VM.</p>
<p>Add the box file so it can be used by Vagrant.</p>
<pre><code>vagrant box add windows_server_insider_docker windows_server_insider_docker_hyperv.box
</code></pre>
<h2 id="boottheinsidervm">Boot the Insider VM</h2>
<p>Now we're using <a href="https://vagrantup.com">Vagrant</a> to boot the Insider VM. I'll use my <a href="https://github.com/StefanScherer/windows-docker-machine">windows-docker-machine</a> Vagrant template which I also use locally on a Mac or Windows 10 laptop.</p>
<pre><code>git clone https://github.com/StefanScherer/windows-docker-machine
cd windows-docker-machine
vagrant plugin install vagrant-reload

vagrant up --provider hyperv insider
</code></pre>
<p>This will spin up a VM and creates TLS certificates for the Docker engine running in the Windows Insider Server VM.</p>
<p>You could use it from the Azure VM, but we want to make the nested VM reachable from our laptop.</p>
<p>Now retrieve the IP address of this nested VM to add some port mappings so we can access the nested VM from our local machine.</p>
<pre><code>vagrant ssh-config
</code></pre>
<p>Use the IP address shown for the next commands, eg. 192.168.0.10</p>
<pre><code>netsh interface portproxy add v4tov4 listenport=2376 listenaddress=0.0.0.0 connectport=2376 connectaddress=192.168.0.10
netsh interface portproxy add v4tov4 listenport=9000 listenaddress=0.0.0.0 connectport=9000 connectaddress=192.168.0.10
netsh interface portproxy add v4tov4 listenport=3390 listenaddress=0.0.0.0 connectport=3389 connectaddress=192.168.0.10
</code></pre>
<h2 id="createdockertlsforexternaluse">Create Docker TLS for external use</h2>
<p>As we want to access this Docker engine from our local laptop we have to re-create the TLS certs with the FQDN of the Azure VM.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/insider_in_azure_rdp.png" alt="RDP to Azure and nested VM"></p>
<p>Now RDP into the nested VM through port 3390 from your laptop.</p>
<p>You will see a CMD terminal. Run <code>powershell</code> to enter a PowerShell terminal.</p>
<p>Run the <code>create-machine.ps1</code> provision script again with the IP address and the FQDN of the Azure VM. Also specify the path of your local home directory (in my case <code>-machineHome /Users/stefan</code>) to make the docker-machine configuration work.</p>
<pre><code>C:\Users\demo\insider-docker-machine\scripts\create-machine.ps1 -machineHome /Users/stefan -machineName az-insider -machineIp 1.2.3.4 -machineFqdn az-insider-01.westeurope.cloudapp.azure.com
</code></pre>
<h2 id="rundockercontainers">Run Docker containers</h2>
<p>You can copy the generated TLS certificates from the nested VM through the RDP session back to your home directory in <code>$HOME/.docker/machine/machines</code> folder.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/insider_in_azure-1.png" alt="insider_in_azure-1"></p>
<p>Then you can easily switch the Docker environment variables locally on your</p>
<p>Mac:</p>
<pre><code>eval $(docker-machine env az-insider)
</code></pre>
<p>or Windows:</p>
<pre><code>docker-machine env az-insider | iex
</code></pre>
<p>Now you should be able to run Docker commands like</p>
<pre><code>docker images
docker run -it microsoft/nanoserver-insider cmd
</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>We have used a lot of tools to create this setup. If you do this only once it seems to be more step than needed. But keep in mind the Insider builds are shipped regularly so you will do some steps again and again.</p>
<p>To repeat some of these steps tools like Packer and Vagrant can help you go faster building VM's as Docker helps you go faster to ship your apps.</p>
<ul>
<li>Packer helps you repeat building a VM from new ISO.</li>
<li>Vagrant helps you repeat booting fresh VMs. Destroy early and often. Rebuild is cheap.</li>
<li>Docker helps you repeat creating and running applications.</li>
</ul>
<p>If you have another approach to run Insider builds in Azure please let me know. I love to hear your story. Please use the comments below if you have questions or want to share your setup.</p>
<p>If you liked this blog post please share it with your friends. You can follow me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a> to stay updated with Windows containers.</p>
</div>]]></content:encoded></item><item><title><![CDATA[A sneak peek at LCOW]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Last week a major <a href="https://github.com/moby/moby/pull/34859">pull request</a> to support Linux Containers on Windows (LCOW) has landed in master branch of the Docker project. With that feature enabled you will be able to run <strong>both Linux and Windows containers side-by-side</strong> with a single Docker engine.</p>
<p>So let's have a look how a</p></div>]]></description><link>https://stefanscherer.github.io/sneak-peek-at-lcow/</link><guid isPermaLink="false">5a64a75ee5611a0001acf91f</guid><category><![CDATA[Docker]]></category><category><![CDATA[LCOW]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Windows 10]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Sun, 21 Jan 2018 15:30:58 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Last week a major <a href="https://github.com/moby/moby/pull/34859">pull request</a> to support Linux Containers on Windows (LCOW) has landed in master branch of the Docker project. With that feature enabled you will be able to run <strong>both Linux and Windows containers side-by-side</strong> with a single Docker engine.</p>
<p>So let's have a look how a Windows 10 developer machine will look like in near future.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/lcow-marked.jpg" alt="LCOW on Windows 10"></p>
<ul>
<li>The Docker command <code>docker ps</code> lists all your running Linux and Windows containers.</li>
<li>You can use volumes to share data between containers and the host.</li>
<li>The containers can talk to each other over the container networks.</li>
<li>You can publish ports to your host and use localhost. But wait, this is still a Windows Insider feature coming to Windows 10 1803 release.</li>
</ul>
<h2 id="runninglinuxcontainers">Running Linux containers</h2>
<p>At the moment you need to specify the <code>--platform</code> option to pull Linux images. This option is also needed when the specific Docker images is a multi-arch image for both Linux and Windows.</p>
<pre><code>docker pull --platform linux alpine
</code></pre>
<p>Once you have pulled Linux images you can run them without the <code>--platform</code> option.</p>
<pre><code>docker run alpine uname -a
</code></pre>
<p>To allow Windows run Linux containers a small Hyper-V VM is needed. The LinuxKit project provides an image for LCOW at <a href="https://github.com/linuxkit/lcow">https://github.com/linuxkit/lcow</a>.</p>
<h1 id="sharedvolumes">Shared volumes</h1>
<p>Let's see how containers of different platforms can share data in a simple way. You can bind mount a volume into Linux and Windows containers.</p>
<p><img src="https://stefanscherer.github.io/content/images/2018/01/lcow-in-action.gif" alt="LCOW in action with shared volumes"></p>
<p>The following example shares a folder from the host with a Linux and Windows container.</p>
<p>First create a folder on the Windows 10 host:</p>
<pre><code>cd \
mkdir host
</code></pre>
<h3 id="runalinuxcontainer">Run a Linux container</h3>
<p>On the Windows 10 host run a Linux container and bind mount the folder as <code>/test</code> in the Linux container.</p>
<pre><code>docker run -it -v C:\host:/test alpine sh
</code></pre>
<p>In the Linux container create a file in that mounted volume.</p>
<pre><code>uname -a &gt; test/hello-from-linux.txt
</code></pre>
<h3 id="runawindowscontainer">Run a Windows container</h3>
<p>On the Windows 10 host run a Windows container and bind mount the folder as <code>C:\test</code> in the Windows container.</p>
<pre><code>docker run -i -v C:\host:C:\test microsoft/nanoserver:1709 cmd
</code></pre>
<p>In the Windows container create a file in that mounted volume.</p>
<pre><code>ver &gt; test\hello-from-windows.txt
</code></pre>
<h3 id="result">Result</h3>
<p>On the Windows 10 host list the files in the shared folder</p>
<pre><code>PS C:\&gt; dir host


    Directory: C:\host


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
-a----        1/21/2018   4:32 AM             85 hello-from-linux.txt
-a----        1/21/2018   4:33 AM             46 hello-from-windows.txt
</code></pre>
<p>This is super convenient for development environments to share configuration files or even source code.</p>
<h1 id="draftingmixedworkloads">Drafting mixed workloads</h1>
<p>With Docker Compose you can spin up a mixed container environment. I just did these first steps to spin up a Linux and Windows web server.</p>
<pre><code class="language-yaml">version: &quot;3.2&quot;

services:

  web1:
    image: nginx
    volumes:
      - type: bind
        source: C:\host
        target: /test
    ports:
      - 80:80

  web2:
    image: stefanscherer/hello-dresden:0.0.3-windows-1709
    volumes:
      - type: bind
        source: C:\host
        target: C:\test
    ports:
      - 81:3000

networks:
  default:
    external:
      name: nat
</code></pre>
<p>Think of a Linux database and a Window front end, or vice versa...</p>
<h1 id="buildyourownlcowtestenvironment">Build your own LCOW test environment</h1>
<p>If you want to try LCOW yourself I suggest to spin up a fresh Windows 10 1709 VM.</p>
<h2 id="azure">Azure</h2>
<p>I have tested LCOW with a Windows 10 1709 VM in Azure. Choose a V3 machine to have nested hypervisor which you will need to run Hyper-V containers.</p>
<h3 id="containersandhyperv">Containers and Hyper-V</h3>
<p>Enable the Containers feature and Hyper-V feature:</p>
<pre><code>Enable-WindowsOptionalFeature -Online -FeatureName containers -All -NoRestart
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All -NoRestart
</code></pre>
<h3 id="linuxkit">LinuxKit</h3>
<p>Now install the LinuxKit image for LCOW. I have catched the latest from a CircleCI artifact, but soon there will be a new release in the <a href="https://github.com/linuxkit/lcow/releases">linuxkit/lcow</a> repo.</p>
<pre><code>Invoke-WebRequest -OutFile &quot;$env:TEMP\linuxkit-lcow.zip&quot; &quot;https://23-111085629-gh.circle-artifacts.com/0/release.zip&quot;
Expand-Archive -Path &quot;$env:TEMP\linuxkit-lcow.zip&quot; -DestinationPath &quot;$env:ProgramFiles\Linux Containers&quot; -Force
</code></pre>
<h3 id="dockernightlybuild">Docker nightly build</h3>
<p>Now download and install the Docker engine. As this pull request only landed in master branch we have to use the nightly build for now.</p>
<pre><code>Invoke-WebRequest -OutFile &quot;$env:TEMP\docker-master.zip&quot; &quot;https://master.dockerproject.com/windows/x86_64/docker.zip&quot;
Expand-Archive -Path &quot;$env:TEMP\docker-master.zip&quot; -DestinationPath $env:ProgramFiles -Force
</code></pre>
<p>The next command installs the Docker service and enables the experimental features.</p>
<pre><code>. $env:ProgramFiles\docker\dockerd.exe --register-service --experimental
</code></pre>
<p>Set the PATH variable to have the Docker CLI available.</p>
<pre><code>[Environment]::SetEnvironmentVariable(&quot;Path&quot;, $env:Path + &quot;;$($env:ProgramFiles)\docker&quot;, [EnvironmentVariableTarget]::Machine)
</code></pre>
<p>Now reboot the machine to finish the Containers and Hyper-V installation. After the reboot the Docker engine should be up and running and the Docker CLI can be used from the PowerShell terminal.</p>
<h2 id="localvagrantenvironment">Local Vagrant environment</h2>
<p>If you have <a href="https://vagrantup.com">Vagrant</a> installed with Hyper-V or VMware as your hypervisor, you can spin up a local test environment with a few commands.</p>
<p>First clone my GitHub repo <a href="https://github.com/StefanScherer/docker-windows-box">docker-windows-box</a> which has a LCOW environment to play with.</p>
<pre><code>git clone https://github.com/StefanScherer/docker-windows-box
cd docker-windows-box
cd lcow
vagrant up
</code></pre>
<p>This will download the Vagrant base box if needed, spins up the Windows 10 VM and automatically installs all features shown above.</p>
<h1 id="conclusion">Conclusion</h1>
<p>With all these new Docker features coming to Windows in the next few months, Windows 10 is evolving to the most interesting developer platform in 2018.</p>
<p>Imagine what's possible: Use a <code>docker-compose.yml</code> to spin up a mixed scenario with Linux and Windows containers, live debug your app from Visual Studio Code, and much more.</p>
<p>If you liked this blog post please share it with your friends. You can follow me on Twitter <a href="https://twitter.com/stefscherer">@stefscherer</a> to stay updated with Windows containers.</p>
</div>]]></content:encoded></item><item><title><![CDATA[PoC: How to build images for 1709 without 1709]]></title><description><![CDATA[<div class="kg-card-markdown"><p>First of all: Happy Halloween! In this blog post you'll see some spooky things - or magic? Anyway I found a way to build Windows Docker images based on the new 1709 images without running on 1709. Sounds weird?</p>
<blockquote>
<p><strong>Disclaimer:</strong> The tools and described workflow to build such images on</p></blockquote></div>]]></description><link>https://stefanscherer.github.io/poc-build-images-for-1709-without-1709/</link><guid isPermaLink="false">59f90ee4f830c70001a9b8f1</guid><category><![CDATA[Docker]]></category><category><![CDATA[Windows Server 1709]]></category><category><![CDATA[windows-containers]]></category><category><![CDATA[AppVeyor]]></category><category><![CDATA[multi-arch]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Tue, 31 Oct 2017 23:55:00 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>First of all: Happy Halloween! In this blog post you'll see some spooky things - or magic? Anyway I found a way to build Windows Docker images based on the new 1709 images without running on 1709. Sounds weird?</p>
<blockquote>
<p><strong>Disclaimer:</strong> The tools and described workflow to build such images on old Windows Server versions may break at any time. It works for me and some special cases, but it does not mean it works for any other use-case.</p>
</blockquote>
<h2 id="the20161709gap">The 2016 &lt;-&gt; 1709 gap</h2>
<p>As you might know from my <a href="https://stefanscherer.github.io/docker-on-windows-server-1709/">previous blog post</a> there is a gap between the old and new Windows images. You cannot pull the new 1709 Docker images on current Windows Server 2016. This means you also cannot build images without updating your build machines to Windows Server 1709.</p>
<h2 id="appveyor">AppVeyor</h2>
<p>My favorite CI service for Windows is AppVeyor. They provide a Windows Server 2016 build agent with Docker and the latest base images installed. So it is very simple and convenient to build your Windows Docker images there. For example all my <a href="https://github.com/StefanScherer/dockerfiles-windows">dockerfiles-windows</a> Dockerfiles are built there and the images are pushed to Docker Hub.</p>
<p>I guess it will take a while until we can choose another build agent to start building for 1709 there.</p>
<p>But what should I do in the meantime?</p>
<ul>
<li>Should I build all 1709 images manually on a local VM?</li>
<li>Or spin up a VM in Azure? It is possible since today.</li>
</ul>
<p>But then I don't have the nice GitHub integration. And I have to do all the maintenance of a CI server (cleaning up disk space and so on) myself. Oh I don't want to go that way.</p>
<h2 id="dockerimageshavelayers">Docker images have layers</h2>
<p>Let's have a closer look at how a Docker image looks like. Each Docker image contains of one or more layers. Each layer is read-only. Any change will be done in a new layer on top of the underlying ones.</p>
<p>For example the Windows Docker image of a Node.js application looks more or less like this:</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/11/windows_image_layers-2.png" alt="windows_image_layers-2"></p>
<p>At the bottom you find the Windows base image, then we add the Node.js runtime. Then we can add our application code on top of that. This is how a Dockerfile works. Every FROM, RUN, ... is an extra layer.</p>
<p>Technically all layers are just tarballs with files and directories in it. So when the application and framework layer are independent from the OS system layer it should be possible to rearrange them with a new OS layer.</p>
<h2 id="rebasedockerimage">Rebase Docker image</h2>
<p>That is what I have tried to find out. I studied the Docker Hub API and wrote a proof of concept to &quot;rebase&quot; a given Windows Docker image to swap the old Windows OS layers with another one.</p>
<p>The tool works only with information from Docker Hub so it only retrieves metadata and pushes a new manifest back to the Docker Hub. This avoids downloading hundreds of megabytes for the old nanoserver images.</p>
<h3 id="usecases">Use cases</h3>
<ul>
<li>Easily apply Windows Updates to an existing Windows app in seconds. Only the update layer will be swapped.</li>
<li>Provide your app for all available Windows Update layers to avoid download.</li>
<li>Sync multiple images based on different Windows Update layers to the current to avoid downloading several different udpate layers for a multi-container application.</li>
<li>Create images for Server 1709 without having a machine for it.</li>
</ul>
<h3 id="limits">Limits</h3>
<ul>
<li>You cannot move an app from a windowsservercore image to the nanoserver image.</li>
<li>You also cannot move PowerShell scripts into the 1709 nanoserver image as there is no PowerShell installed.</li>
<li>Your framework or application layer may has modified the Windows registry at build time. It then carries the old registry that may not fit to new base layer.</li>
<li>Moving such old application layers on top of new base layers is some kind of time travel. Be warned that this tool may create corrupt images.</li>
</ul>
<p>You can find the <a href="https://github.com/StefanScherer/rebase-docker-image">rebase-docker-image</a> tool on GitHub. It is a Node.js command line tool which is also available on NPM.</p>
<p>The usage looks like this:</p>
<pre><code>$ rebase-docker-image \
    stefanscherer/hello-freiburg:windows \
    -t stefanscherer/hello-freiburg:1709 \
    -b microsoft/nanoserver:1709
</code></pre>
<p>You specify the existing image, eg. &quot;stefanscherer/hello-freiburg:windows&quot; which is based on nanoserver 10.0.14393.x.</p>
<p>With the <code>-t</code> option you specify the target image name that where the final manifest should be pushed.</p>
<p>The <code>-b</code> option specifies the base image you want to use, so most of the time the &quot;microsoft/nanoserver:1709&quot; image.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/11/rebase_docker_image.png" alt="rebase_docker_image"></p>
<p>When we run the tool it does its job in only a few seconds:</p>
<pre><code>Retrieving information about source image stefanscherer/hello-freiburg:windows
Retrieving information about source base image microsoft/nanoserver:10.0.14393.1715
Retrieving information about target base image microsoft/nanoserver:1709
Rebasing image
Pushing target image stefanscherer/hello-freiburg:1709
Done.
</code></pre>
<p>Now on a Windows Server 1709 we can run the application.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/11/hello-freiburg-1709.png-shadow.png" alt="hello-freiburg-1709.png-shadow"></p>
<p>I tried this tool with some other Windows Docker images and was able to rebase the golang:1.9-nanoserver image to have a Golang build environment for 1709 without rebuilding the Golang image by myself.</p>
<p>But I also found situations where the rebase didn't work, so don't expect it to work everywhere.</p>
<h2 id="appveyorcipipeline">AppVeyor CI pipeline</h2>
<p>I also want to show you a small CI pipeline using AppVeyor to build a Windows image with <code>curl.exe</code> installed and provide two variants of that Docker image, one for the old nanoserver and one with the nanoserver:1709 image.</p>
<p>The <a href="https://github.com/StefanScherer/dockerfiles-windows/blob/master/curl/Dockerfile">Dockerfile</a> uses a multi-stage build. In the first stage we download and extract curl and its DLL's. The second stage starts again with the empty nanoserver (the fat one for Windows Server 2016) and then we just COPY deploy the binary into the fresh image. An ENTRYOINT finishes the final image.</p>
<pre><code class="language-Dockerfile">FROM microsoft/nanoserver AS download
ENV CURL_VERSION 7.56.1
WORKDIR /curl
ADD https://skanthak.homepage.t-online.de/download/curl-$CURL_VERSION.cab curl.cab
RUN expand /R curl.cab /F:* .

FROM microsoft/nanoserver
COPY --from=download /curl/AMD64/ /
COPY --from=download /curl/CURL.LIC /
ENTRYPOINT [&quot;curl.exe&quot;]
</code></pre>
<p>This image can be built on AppVeyor and pushed to the Docker Hub.</p>
<p>The <a href="https://github.com/StefanScherer/dockerfiles-windows/blob/master/curl/push.ps1">push.ps1</a> script pushes this image to Docker Hub.</p>
<pre><code>docker push stefanscherer/curl-windows:$version-2016
</code></pre>
<p>Then the rebase tool will be installed and the 1709 variant will be pushed as well to Docker Hub.</p>
<pre><code>npm install -g rebase-docker-image
rebase-docker-image `
  stefanscherer/curl-windows:$version-2016 `
  -t stefanscherer/curl-windows:$version-1709 `
  -b microsoft/nanoserver:1709
</code></pre>
<p>To provide my users the best experience I also draft a manifest list, just like we did for multi-arch images at the <a href="https://stefanscherer.github.io/cross-build-nodejs-with-docker/">Captains Hack day</a>. The final &quot;tag&quot; then contains both Windows OS variants.</p>
<p>On Windows you can use Chocolatey to install the manifest-tool. In the future this feature will be integrated into the Docker CLI as &quot;docker manifest&quot; command.</p>
<pre><code>choco install -y manifest-tool
manifest-tool push from-spec manifest.yml
</code></pre>
<p>The <a href="https://github.com/StefanScherer/dockerfiles-windows/blob/master/curl/manifest.yml">manifest.yml</a> lists both images and joins them together to the final <code>stefanscherer/curl-windows</code> image.</p>
<pre><code class="language-yaml">image: stefanscherer/curl-windows:7.56.1
tags: ['7.56', '7', 'latest']
manifests:
  -
    image: stefanscherer/curl-windows:7.56.1-2016
    platform:
      architecture: amd64
      os: windows
  -
    image: stefanscherer/curl-windows:7.56.1-1709
    platform:
      architecture: amd64
      os: windows
</code></pre>
<p>So on both Windows Server 2016 and Windows Server 1709 the users can run the same image and it will work.</p>
<pre><code>PS C:\Users\demo&gt; docker run stefanscherer/curl-windows
curl: try 'curl --help' or 'curl --manual' for more information
</code></pre>
<p>This requires the next Docker 17.10 EE version to work correctly, but it should be available soon. With older Docker engines it may pick the wrong version of the list of Docker images and fail running it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This way to &quot;rebase&quot; Docker images works astonishingly good, but keep in mind that this is not a general purpose solution. It is always better to use the correct version on the host to rebuild your Docker images the official way.</p>
<p>Please use the comment below if you have further questions or share what you think about that idea.</p>
<p>Stefan<br>
<a href="https://twitter.com/stefscherer">@stefscherer</a></p>
</div>]]></content:encoded></item><item><title><![CDATA[A closer look at Docker on Windows Server 1709]]></title><description><![CDATA[<div class="kg-card-markdown"><p>Today Microsoft has released Windows Server 1709 in Azure. The ISO file is also available in the MSDN subscription to build local VM's. But spinning up a cloud VM makes it easier for more people.</p>
<p>So let's go to Azure and create a new machine. The interesting VM for me</p></div>]]></description><link>https://stefanscherer.github.io/docker-on-windows-server-1709/</link><guid isPermaLink="false">59f8a705f830c70001a9b8ee</guid><category><![CDATA[Docker]]></category><category><![CDATA[Windows Server 1709]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Tue, 31 Oct 2017 23:18:14 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>Today Microsoft has released Windows Server 1709 in Azure. The ISO file is also available in the MSDN subscription to build local VM's. But spinning up a cloud VM makes it easier for more people.</p>
<p>So let's go to Azure and create a new machine. The interesting VM for me is &quot;Windows Server, version 1709 with Containers&quot; as it comes with Docker preinstalled.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/Bildschirmfoto-2017-10-31-um-22.22.35.png" alt="azure search for 1709"></p>
<p>After a few minutes you can RDP into the machine. But watch out, it is only a Windows Server Core, so there is no full desktop. But for a Docker host this is good enough.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/docker1709-01.png-shadow.png" alt="Docker 17.06.1 EE preinstalled"></p>
<p>As you can see the VM comes with the latest Docker 17.06.1 EE and the new 1709 base images installed.</p>
<h2 id="smaller1709baseimages">Smaller &quot;1709&quot; base images</h2>
<p>On great news is that the base images got smaller. For comparison here are the images of a Windows Server 2016:</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/docker2016-01.png-shadow.png" alt="Windows Server 2016 images"></p>
<p>So with Windows Server 1709 the WindowsServerCore image is only 1/2 the size of the original. And for the NanoServer image is about 1/4 with only 93 MB on the Docker Hub.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/docker-hub-nanoserver.png" alt="docker-hub-nanoserver"></p>
<p>That makes the NanoServer image really attractive to deploy modern microservices with it. As you can see, the &quot;latest&quot; tag is still pointing to the old image. As the 1709 release is a semi-annual release supported for the next 18 months and the current Windows Server 2016 is the LTS version, the latest tags still remain to the older, thicker images.</p>
<p>So when you want to go small, then use the &quot;1709&quot; tags:</p>
<ul>
<li>microsoft/windowsservercore:1709</li>
<li>microsoft/nanoserver:1709</li>
</ul>
<h2 id="whereispowershell">Where is PowerShell?</h2>
<p>The small size of the NanoServer image comes with a cost: There is no PowerShell installed inside the NanoServer image.</p>
<p>So is that really a problem?</p>
<p>Yes and no. Some people have started to write Dockerfiles and installed software using PowerShell in the <code>RUN</code> instructions. This will be a breaking change.</p>
<p>The good news is that there will be a PowerShell Docker image based on the small nanoserver:</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/docker-hub-powershell.png" alt="docker-hub-powershell"></p>
<p>Currently there is PowerShell 6.0.0 Beta 9 available and you can run it with</p>
<pre><code>docker run -it microsoft/powershell:nanoserver
</code></pre>
<p>As you can see PowerShell takes 53 MB on top of the 93 MB nanoserver.</p>
<p>So if you really want to deploy your software with PowerShell, then you might use this base image in your <code>FROM</code> instruction.</p>
<p>But if you deploy a Golang, Node.js, .NET Core application you probably don't need PowerShell.</p>
<p>My experience with Windows Dockerfiles is that the common tasks are</p>
<ul>
<li>downloading a file, zip, tarball from the internet</li>
<li>extracting the archive</li>
<li>Setting an environment variable like PATH</li>
</ul>
<p>These steps could be done with tools like curl (yes, I think of the real one and not the curl alias in PowerShell :-) and some other tools like unzip, tar, ... that are way smaller than the complete PowerShell runtime.</p>
<p>I did a small proof of concept to put some of the tools mentioned into a NanoServer image. You can find the Dockerfile an others in my <a href="https://github.com/StefanScherer/dockerfiles-windows">dockerfiles-windows</a> GitHub repo.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/docker-hub-busybox-windows.png" alt="docker-hub-busybox-windows"></p>
<p>As you can see it only takes about 2 MB to have download and extracting tools. The remaining <code>cmd.exe</code> in the NanoServer image is still good enough to run these tools in the <code>RUN</code> instructions of a Dockerfile.</p>
<h2 id="multistagebuilds">Multi-stage builds</h2>
<p>Another approach to build small images based on NanoServer comes with Docker 17.06. You can use multi-stage builds which brings you so much power and flexibility into a Dockerfile.</p>
<p>You can start with a bigger image, for example the PowerShell image or even the much bigger WindowServerCore image. In that stage of the Dockerfile you have all scripting languages or even build tools or MSI support.</p>
<p>The final stage then uses the smallest NanoServer use <code>COPY</code> deploy instructions for your production image.</p>
<h2 id="caniusemyoldimagesonserver1709">Can I use my old images on Server 1709?</h2>
<p>Well, it depends. Let's test this with a popular application from <a href="http://portainer.io">portainer.io</a>. When we try to run the application on a Windows Server 1709 we get the following error message: <em>The operating system of the container does not match the operating sytem of the host.</em></p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/portainer-on-1709.png-shadow.png" alt="portainer-on-1709.png-shadow"></p>
<p>We can make it work when we run the old container with Hyper-V isolation:</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/portainer-hyperv.png-shadow.png" alt="portainer-hyperv.png-shadow"></p>
<p>For the Hyper-V isolation we need Hyper-V installed. This works in Azure with the v3 machines that allows nested virtualization. If you are using Windows 10 1709 with Hyper-V then you also can run old images in Docker 4 Windows.</p>
<p>But there are many other situations where you are out of luck:</p>
<ul>
<li>other cloud providers that does not have nested virtualization</li>
<li>VirtualBox</li>
</ul>
<p>So my recommendation is to create new Docker images based on 1709 that can be used with Windows 10 1709, or Windows Server 1709 even without Hyper-V. Another advantage is that your users have much smaller downloads and can run your apps much faster.</p>
<h2 id="caniusethe1709imagesonserver2016">Can I use the 1709 images on Server 2016?</h2>
<p><strong>No.</strong> If you try to run one of the 1709 based images on a Windows Server 2016 you see the following error message. Even running it with <code>--isolation=hyperv</code> does not help here as the underlying VM compute of your host does not have all the new features needed.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/nano1709-on-2016.png-shadow.png" alt="nano1709-on-2016.png-shadow"></p>
<h2 id="conclusion">Conclusion</h2>
<p>With Docker on Windows Server 1709 the container images get much smaller. Your downloads are faster and smaller, the containers start faster. If you're interested in Windows Containers then you should switch over to the new server version. The upcoming Linux Containers on Windows feature will run only on Windows 10 1709/Windows Server 1709 and above.</p>
<p>As a software vendor providing Windows Docker images you should provide both variants as people still use Windows 10 and Windows Server 2016 LTS. In a <a href="https://stefanscherer.github.io/poc-build-images-for-1709-without-1709/">following blog post</a> I'll show a way that makes it easy for your users to just run your container image regardless the host operating system they have.</p>
<p>I hope you are as excited as I am about the new features of the new Windows Server 1709. If you have questions feel free to drop a comment below.</p>
<p>Stefan<br>
<a href="https://twitter.com/stefscherer">@stefscherer</a></p>
</div>]]></content:encoded></item><item><title><![CDATA[Cross-build a Node.js app with Docker and deploy to IBM Cloud]]></title><description><![CDATA[<div class="kg-card-markdown"><p>After the DockerCon EU and the Moby Summit in Copenhagen last week we also had an additional Docker Captain's Hack Day. After introducing our current projects to the other Captains we also had time to work together on some ideas.</p>
<blockquote>
<p><em>&quot;Put all Captains available into a room, feed them</em></p></blockquote></div>]]></description><link>https://stefanscherer.github.io/cross-build-nodejs-with-docker/</link><guid isPermaLink="false">59f7701f71f6240001940592</guid><category><![CDATA[Docker]]></category><category><![CDATA[Node.js]]></category><category><![CDATA[multi-arch]]></category><dc:creator><![CDATA[Stefan Scherer]]></dc:creator><pubDate>Mon, 30 Oct 2017 22:37:03 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p>After the DockerCon EU and the Moby Summit in Copenhagen last week we also had an additional Docker Captain's Hack Day. After introducing our current projects to the other Captains we also had time to work together on some ideas.</p>
<blockquote>
<p><em>&quot;Put all Captains available into a room, feed them well and see what's happening.&quot;</em></p>
</blockquote>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/captains-hack-day.jpg" alt="captains-hack-day"></p>
<h2 id="modernizingswarmvisualizer">Modernizing Swarm Visualizer</h2>
<p>One of the ideas was Swarm Visualizer 2.0. Michael Irwin came up with the idea to rewrite the current Visualizer to be event driven, use a modern React framework and cleanup the code base.</p>
<p>The old one uses a dark theme and shows lots of details for the services with small fonts.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/Bildschirmfoto-2017-10-30-um-19.47.10.png" alt="old swarm visualizer"></p>
<p>Here's a screenshot of an early version of the new UI. With a click on one of the tasks you get more details about that task and its service. All information is updated immediately when you update the service (eg. add or remove labels).</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/Bildschirmfoto-2017-10-30-um-19.50.18.png" alt="new swarm visualizer"></p>
<p>You can try this new Swarm visualizer yourself with the following command:</p>
<pre><code>docker container run \
  --name swarm-viz \
  -p 3000:3000 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  mikesir87/swarm-viz
</code></pre>
<p>I joined Michael's table as I was curious if we can have this visualizer for Windows, too. Especially the new Windows Server 1709 that makes mapping the Docker API into a Windows container as easy as on Linux.</p>
<p>In this blog post I focus on how to build a Node.js app with Docker and don't look into the details of the app itself. I'll show how to improve the Dockerfile to build for multiple platforms and finally how to build a CI pipeline for that. You can find the project on <a href="https://github.com/mikesir87/swarm-viz">github.com/mikesir87/swarm-viz</a>.</p>
<h2 id="initialdockerfile">Initial Dockerfile</h2>
<p>The application is built inside a Docker container. So you even can build it without any developer tools installed, you only need Docker.</p>
<p>Let's have a look at the first version of the Dockerfile for the Linux image. It is a multi-stage build with three stages:</p>
<pre><code class="language-Dockerfile"># Build frontend
FROM node:8.7-alpine as frontend
WORKDIR /app
COPY client/package.json .
RUN npm install
COPY client/ .
RUN npm run build

# Build backend
FROM node:8.7-alpine as backend
WORKDIR /app
COPY api/package.json .
RUN npm install
COPY api/ .
RUN npm run build

# Put them together
FROM node:8.7-alpine
EXPOSE 3000
WORKDIR /app
COPY api/package.json .
RUN npm install --production
COPY --from=backend /app/dist /app/dist
COPY --from=frontend /app/build /app/build
CMD node /app/dist/index.js
</code></pre>
<p>The first stage uses <code>FROM node:8.7-alpine</code> to build the frontend in a container.</p>
<p>The second stage builds the backend in another Alpine container. During that build you also need some development dependencies that aren't needed for the final image.</p>
<p>In the third stage only the dependencies that are relevant at runtime are installed with <code>npm install --production</code>. All artifacts needed from the other stages are also copied into the final image.</p>
<h2 id="makefrommoreflexibleforwindows">Make FROM more flexible for Windows</h2>
<p>I tried to build the app for Windows Server 1709 and had to create a second Dockerfile as I have to use another <code>FROM</code> as node does not have a Windows variant in the official images. And Windows Server 1709 just came out so I had to create a Node.js base image for Windows myself.</p>
<p>So what I did was copying the Dockerfile to Dockerfile.1709 and changed all the</p>
<pre><code>FROM node:8.7-alpine
</code></pre>
<p>lines into</p>
<pre><code>FROM stefanscherer/node-windows:1709
</code></pre>
<p>But now we have duplicated the Dockerfile &quot;code&quot; for only this little difference.</p>
<p>Fortunately you now can use build arguments for the <code>FROM</code> instruction. So with only a little change we can have <strong>ONE</strong> Dockerfile for Linux and Windows.</p>
<pre><code>ARG node=node:8.7-alpine
FROM $node as frontend
</code></pre>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/add-arg.png" alt="add-arg"></p>
<p>On Linux you still can build the image as before without any change.</p>
<p>On Windows I now was able to use this Dockerfile with</p>
<pre><code>docker image build -t viz `
  --build-args node=stefanscherer/node-windows:1709 .
</code></pre>
<p>and use a Windows Node.js base image for all stages. <a href="https://github.com/mikesir87/swarm-viz/pull/2">First pull request</a> done. Check! </p>
<p>And running the manually built image in Windows Server 1709 looks very similar to Linux:</p>
<pre><code>docker container run `
  -p 3000:3000 `
  -u ContainerAdministrator `
  -v //./pipe/docker_engine://./pipe/docker_engine `
  viz
</code></pre>
<h2 id="goingmultiarch">Going multi-arch</h2>
<p>We showed the Windows Swarm visualizer to other Captains and we discussed how to go to more platforms. Phil Estes, a very active member of the Docker community who's helping push the multi-architecture support in Docker forward and the maintainer of the <a href="https://github.com/estesp/manifest-tool">manifest-tool</a>, commented:</p>
<p><em>With Golang it is easy to build multi-arch images, just cross-build a static binary with <code>GOARCH=bar go build app.go</code> and copy the binary in an empty <code>FROM scratch</code> image.</em></p>
<p>Hm, we use Node.js here, so what has to be done instead?</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/captain-hack-day-1.jpg" alt="captain-hack-day-1"></p>
<p>Well, instead of the <code>scratch</code> image we need the <code>node</code> image for the Node.js runtime. So we had to <strong>choose the desired architecture</strong> and then copy all sources and dependencies into that image.</p>
<p>Our Node.js application uses Express, Dockerode and some other dependencies, that are platform independent. So this simple copy approach should do it, we thought.</p>
<p>We added another build stage in the Dockerfile where we switch to the desired platform. You may know, the <code>node</code> image on Docker Hub is already a multi-arch image. But in this case we want to build - let's say on Linux/amd64 - for another platform like the IBM s390 mainframe.</p>
<p>With another build argument to specify the target platform for the final stage we came up with this:</p>
<pre><code class="language-Dockerfile">ARG node=node:8.7-alpine
ARG target=node:8.7-alpine

FROM $node as frontend
...

FROM $target
EXPOSE 3000
COPY --from=proddeps /app /app
CMD node /app/dist/index.js
</code></pre>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/add-target.png" alt="add-target"></p>
<p>As Phil works for IBM he could easily verify our approach. We built an IBM version of the Swarm visualizer with</p>
<pre><code>docker image build -t mikesir87/swarm-viz \
  --build-arg target=s390x/node:8.7 .
</code></pre>
<p>and pushed it to the Docker Hub. Phil then pulled and started the container in IBM Cloud and showed us the visualizer UI. Hurray!</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/deploy-to-ibm.jpg" alt="deploy-to-ibm"></p>
<p>The <a href="https://github.com/mikesir87/swarm-viz/pull/2">second pull request</a> was accepted. Check! </p>
<p>Now we needed some more automation to build and push the Docker images.</p>
<h2 id="addingamultiarchcipipeline">Adding a multi-arch CI pipeline</h2>
<p>I've done that several times for my Raspberry Pi projects, so cherry-picked the relevant parts from other repos. For the CI pipeline we choose Travis CI, but any other CI cloud service could be used that allows multi-stage builds.</p>
<p>The <a href="https://github.com/mikesir87/swarm-viz/blob/master/.travis.yml">.travis.yml</a> uses a matrix build for all architectures. Currently we're building it for only two platforms:</p>
<pre><code class="language-yaml">sudo: required

services:
 - docker

env:
  matrix:
    - ARCH=amd64
    - ARCH=s390x

script:
  - ./travis-build.sh
</code></pre>
<h3 id="build">build</h3>
<p>The <a href="https://github.com/mikesir87/swarm-viz/blob/master/travis-build.sh">travis-build.sh</a> then is called for each architecture of that matrix and we run the corresponding build.</p>
<pre><code>docker image build -t mikesir87/swarm-viz \
    --build-arg target=$ARCH/node:8.7 .
</code></pre>
<h3 id="deploy">deploy</h3>
<p>As a final step in the .travis.yml we push every image to Docker Hub and tag it with the Git commit id. At this early stage of the project this is good enough. Later on you can think of tagged release builds etc.</p>
<p>The <a href="https://github.com/mikesir87/swarm-viz/blob/master/travis-deploy.sh">travis-deploy.sh</a> pushes the Docker image for each architecture to the Docker Hub with a different tag using the <code>$ARCH</code> variable we get from the matrix build.</p>
<pre><code>docker image push &quot;$image:linux-$ARCH-$TRAVIS_COMMIT&quot;
</code></pre>
<p>In the amd64 build we additionally download and use the manifest-tool to push a manifest list with the final tag.</p>
<pre><code>manifest-tool push from-args \
    --platforms linux/amd64,linux/s390x \
    --template &quot;$image:OS-ARCH-$TRAVIS_COMMIT&quot; \
    --target &quot;$image:latest&quot;
</code></pre>
<p>You can verify that the <code>latest</code> tag is already a manifest list with another Docker image provided by Phil</p>
<pre><code>$ docker container run --rm mplatform/mquery mikesir87/swarm-viz
Image: mikesir87/swarm-viz:latest
 * Manifest List: Yes
 * Supported platforms:
   - amd64/linux
   - s390x/linux
</code></pre>
<h2 id="futureimprovements">Future improvements</h2>
<p>In the near future we will also add a Windows build using AppVeyor CI to provide Windows images and also put them into the manifest list. This step would also be needed for Golang projects as you cannot use the empty <code>scratch</code> image on Windows.</p>
<p><img src="https://stefanscherer.github.io/content/images/2017/10/ci-pipeline-1.png" alt="ci-pipeline-1"></p>
<p>If you watch closely we have used <code>node:8.7</code> for the final stage. There is no multi-arch <code>alpine</code> image, so there also is no <code>node:8.7-alpine</code> as multi-arch image. But the maintainers of the official Docker images are working hard to add this missing piece to have small images for all architectures.</p>
<pre><code>$ docker container run --rm mplatform/mquery node:8.7-alpine
Image: node:8.7-alpine
 * Manifest List: Yes
 * Supported platforms:
   - amd64/linux
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>At the end of the Hack day we were really excited how far we came in only a few hours and learned that cross-building Node.js apps with Docker and deploying them as multi-arch Docker images isn't that hard.</p>
<p>Best of all, the users of your Docker images don't have to think about these details. They just can run your image on any platform. Just use the command I showed at the beginning as this already uses the multi-arch variant of the next Swarm visualizer app.</p>
<blockquote>
<p>So give multi-arch a try in your next Node.js project to run your app on any platform!</p>
</blockquote>
<p>If you want to learn more about multi-arch (and you want to see Phil with a bow tie) then I can recommend the <a href="https://dockercon.docker.com/watch/Q2LpoYRL3drmxzWc8yDmn9">Docker Multi-arch All the Things</a> talk from DockerCon EU with Phil Estes and Michael Friis.</p>
<p>In my lastest <a href="https://www.slideshare.net/stefscherer/bauen-und-verteilen-von-multiarch-docker-images-fr-linux-und-windows">multi-arch slidedeck</a> there are also more details about the upcoming <code>docker manifest</code> command that will replace the manifest-tool in the future.</p>
<p>Thanks <a href="https://twitter.com/mikesir87">Michael</a> for coming up with that idea, thanks <a href="https://twitter.com/estesp">Phil</a> for the manifest-tool and testing the visualizer. Thanks <a href="https://twitter.com/quintus23m">Dieter</a> and <a href="https://twitter.com/bretfisher">Bret</a> for the photos. You can follow us on Twitter to see what these Captains are doing next.</p>
<p>Stefan<br>
<a href="https://twitter.com/stefscherer">@stefscherer</a></p>
</div>]]></content:encoded></item></channel></rss>